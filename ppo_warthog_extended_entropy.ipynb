{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhil/anaconda3/envs/ppo/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib widget\n",
    "import torch\n",
    "import gym\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import Normal\n",
    "from env.WarthogEnvSup import WarthogEnv\n",
    "import scipy.signal\n",
    "import time\n",
    "import pandas as pd\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "#torch.manual_seed(100)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, act = nn.ReLU):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [1]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.v = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkCat(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension, act= nn.ReLU):\n",
    "        super(PolicyNetworkCat, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.pi = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        score = self.pi(x)\n",
    "        #probs = F.softmax(score,dim = 1)\n",
    "        dist = torch.distributions.Categorical(logits=score)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkGauss(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension,act = nn.ReLU):\n",
    "        super(PolicyNetworkGauss, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.mu = nn.Sequential(*self.layers)\n",
    "        log_std = -0.5*np.ones(action_dimension, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std), requires_grad=True)\n",
    "    def forward(self, x):\n",
    "        mean = self.mu(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "1\n",
      "Normal(loc: torch.Size([1, 2]), scale: torch.Size([1, 2]))\n",
      "tensor([[-0.6869, -0.5023]], grad_fn=<SubBackward0>)\n",
      "tensor([[0.0560, 0.0523]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhil/anaconda3/envs/ppo/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "#env = gym.make('CartPole-v1')\n",
    "env = WarthogEnv('sim_remote_waypoint.txt', 'entropy_manual.csv')\n",
    "sizes = [128]\n",
    "obs_dimension = env.observation_space.shape\n",
    "action_dimension = env.action_space.shape\n",
    "pi = PolicyNetworkGauss(*obs_dimension, sizes, *action_dimension)\n",
    "o = torch.zeros(1,42)\n",
    "a = torch.as_tensor([0.5, 0.3], dtype=torch.float32)\n",
    "print(o)\n",
    "m = pi(o)\n",
    "logp = m.log_prob(a)\n",
    "print(len(a.shape))\n",
    "print(m)\n",
    "print(logp)\n",
    "print(m.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        #self.act_buf = np.zeros((size,), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size,act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "       # print(rews)\n",
    "       # print(vals)\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "\n",
    "        self.adv_buf = (self.adv_buf - self.adv_buf.mean()) / (self.adv_buf.std()+eps)\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env, seed = 0, buff_size = 128, batch_size = 4, train_time_steps = 10000000, gamma = 0.99, clip_ratio = 0.2, lr_pi = 3e-5, \n",
    "        lr_vf = 1e-3, pi_train_itrs = 4, v_train_itrs = 4, lam = 0.97, max_ep_len = 500, ent_coeff = 0.01):\n",
    "        obs_dim = env.observation_space.shape\n",
    "        action_dim = env.action_space.shape\n",
    "        h_sizes = [64,64]\n",
    "        vi = ValueNetwork(*obs_dim, h_sizes, act=nn.Tanh).to(device)\n",
    "        pi = PolicyNetworkGauss(*obs_dim, h_sizes, *action_dim,act=nn.Tanh).to(device)\n",
    "        data_buff = PPOBuffer(*obs_dim, *action_dim, buff_size)\n",
    "        policy_opt = optim.Adam(pi.parameters(), lr = lr_pi, eps=1e-5)\n",
    "        value_opt = optim.Adam(vi.parameters(), lr = lr_vf, eps=1e-5)\n",
    "        obs = env.reset()\n",
    "        curr_time_step = 0\n",
    "        num_episode = 0\n",
    "        ep_rewards = [0]\n",
    "        ep_steps = [0]\n",
    "        start_time = time.time()\n",
    "        while curr_time_step < train_time_steps: \n",
    "                for t in range(0, buff_size):\n",
    "                        curr_time_step+=1\n",
    "                        with torch.no_grad():\n",
    "                                m = pi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                action = m.sample()\n",
    "                                action = action.reshape((-1,) + action_dim)\n",
    "                                logp = m.log_prob(action).sum(dim=1)\n",
    "                                action = action.cpu().numpy() \n",
    "                                clipped_action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "                                obs_new, rew, done, _ = env.step(clipped_action[0])\n",
    "                                ep_rewards[num_episode]+=rew \n",
    "                                ep_steps[num_episode]+=1\n",
    "                                v = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                        data_buff.store(obs, action, rew, v.cpu().numpy(), logp.cpu().numpy())\n",
    "                        obs = obs_new\n",
    "                        if done or buff_size-1:\n",
    "                                if done:\n",
    "                                        v_ = 0.\n",
    "                                        obs = env.reset()\n",
    "                                        done = False\n",
    "                                        num_episode+=1\n",
    "                                        ep_rewards.append(0)\n",
    "                                        ep_steps.append(0)\n",
    "                                        curr_time = time.time()\n",
    "                                        if num_episode %100 == 0:\n",
    "                                                print(f'episode: {num_episode-1} \\t episode_reward: {np.mean(ep_rewards[-10:-2])} \\t total steps:{curr_time_step}\\t fps\"{curr_time_step/(curr_time-start_time)}\\t avg_ep_steps: {np.mean(ep_steps[-10:-2])}')\n",
    "                                                torch.save(pi, f'./temp_policy/tan_h/manaul_entropy3_ppo_batch_{curr_time_step}_rew_{int(np.mean(ep_rewards[-10:-2]))}.pt')\n",
    "                                else:\n",
    "                                        v_ = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                        v_ = v_.detach().cpu().numpy()\n",
    "                                data_buff.finish_path(v_)\n",
    "                        if(curr_time_step % 100000 == 0):\n",
    "                                np.savetxt(f'./temp_policy/tan_h/avg_rew_{curr_time_step}', ep_rewards, fmt='%f')\n",
    "\n",
    "                data = data_buff.get()\n",
    "                ret, act, adv, o, logp_old= data['ret'].to(device), data['act'].to(device), data['adv'].to(device), data['obs'].to(device), data['logp'].to(device)\n",
    "                for j in range(0, pi_train_itrs):\n",
    "                        indices = np.random.permutation(buff_size)\n",
    "                        start_idx = 0\n",
    "                        while start_idx < buff_size:\n",
    "\n",
    "                                batch_ind = indices[start_idx: start_idx+batch_size]\n",
    "                                ret_b = ret[batch_ind]\n",
    "                                act_b = act[batch_ind]\n",
    "                                adv_b = adv[batch_ind]\n",
    "                                o_b = o[batch_ind]\n",
    "                                logp_old_b = logp_old[batch_ind]\n",
    "                                act_dist = pi(o_b)\n",
    "                                logp = act_dist.log_prob(act_b).sum(dim=1)\n",
    "                                ratio = torch.exp(logp - logp_old_b)\n",
    "                                clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv_b\n",
    "                                loss_pi = -(torch.min(ratio * adv_b, clip_adv)).mean() - ent_coeff* torch.mean(-logp)\n",
    "                                policy_opt.zero_grad()\n",
    "                                loss_pi.backward()\n",
    "                                policy_opt.step()\n",
    "                                val = vi(o_b)\n",
    "                                value_loss = F.mse_loss(val.flatten(), ret_b)\n",
    "                                value_opt.zero_grad()\n",
    "                                value_loss.backward()\n",
    "                                value_opt.step()\n",
    "                                start_idx = start_idx+batch_size\n",
    "        return pi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 99 \t episode_reward: 223.9720322304446 \t total steps:14809\t fps\"532.0475628743206\t avg_ep_steps: 269.875\n",
      "episode: 199 \t episode_reward: 73.84467530566775 \t total steps:51246\t fps\"525.3670227828035\t avg_ep_steps: 102.5\n",
      "episode: 299 \t episode_reward: 529.9797695275213 \t total steps:88619\t fps\"523.408549870139\t avg_ep_steps: 437.5\n",
      "episode: 399 \t episode_reward: 965.2117480727647 \t total steps:126424\t fps\"524.5634566010744\t avg_ep_steps: 481.25\n",
      "episode: 499 \t episode_reward: 1392.3648312238192 \t total steps:153300\t fps\"523.6870801777987\t avg_ep_steps: 438.875\n",
      "episode: 599 \t episode_reward: 731.3680715275223 \t total steps:190616\t fps\"524.6879976632835\t avg_ep_steps: 253.125\n",
      "episode: 699 \t episode_reward: 1044.4827254798665 \t total steps:229038\t fps\"527.6367838224281\t avg_ep_steps: 269.25\n",
      "episode: 799 \t episode_reward: 1839.5243984244325 \t total steps:276755\t fps\"528.5802594038717\t avg_ep_steps: 462.375\n",
      "episode: 899 \t episode_reward: 1640.1771953876764 \t total steps:317297\t fps\"527.9116475528031\t avg_ep_steps: 437.5\n",
      "episode: 999 \t episode_reward: 257.85937119466746 \t total steps:346500\t fps\"527.700147991992\t avg_ep_steps: 116.0\n",
      "episode: 1099 \t episode_reward: 245.34501187768274 \t total steps:361720\t fps\"528.5895542243326\t avg_ep_steps: 83.25\n",
      "episode: 1199 \t episode_reward: 359.7641466084853 \t total steps:378905\t fps\"528.3091887443024\t avg_ep_steps: 100.625\n",
      "episode: 1299 \t episode_reward: 1484.061767534112 \t total steps:411600\t fps\"528.9342373403904\t avg_ep_steps: 412.625\n",
      "episode: 1399 \t episode_reward: 1724.3072821202265 \t total steps:457855\t fps\"529.7496081960338\t avg_ep_steps: 525.0\n",
      "episode: 1499 \t episode_reward: 646.0969181733357 \t total steps:497052\t fps\"530.7790099367744\t avg_ep_steps: 437.5\n",
      "episode: 1599 \t episode_reward: 2316.907306854905 \t total steps:527100\t fps\"530.2362765083027\t avg_ep_steps: 700.0\n",
      "episode: 1699 \t episode_reward: 1812.079756203434 \t total steps:559300\t fps\"529.0822060909244\t avg_ep_steps: 408.75\n",
      "episode: 1799 \t episode_reward: 822.2145949035504 \t total steps:600600\t fps\"526.9832964753858\t avg_ep_steps: 277.875\n",
      "episode: 1899 \t episode_reward: 686.9061912011847 \t total steps:629422\t fps\"526.1003171381552\t avg_ep_steps: 210.25\n",
      "episode: 1999 \t episode_reward: 1217.9215457812234 \t total steps:661500\t fps\"525.7680195886334\t avg_ep_steps: 361.875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/akhil/warthog_rl/ppo_warthog_extended_entropy.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akhil/warthog_rl/ppo_warthog_extended_entropy.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39m#python.dataScience.textOutputLimit = 0\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akhil/warthog_rl/ppo_warthog_extended_entropy.ipynb#ch0000008?line=1'>2</a>\u001b[0m pi \u001b[39m=\u001b[39m ppo(env)\n",
      "\u001b[1;32m/home/akhil/warthog_rl/ppo_warthog_extended_entropy.ipynb Cell 9\u001b[0m in \u001b[0;36mppo\u001b[0;34m(env, seed, buff_size, batch_size, train_time_steps, gamma, clip_ratio, lr_pi, lr_vf, pi_train_itrs, v_train_itrs, lam, max_ep_len, ent_coeff)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/warthog_rl/ppo_warthog_extended_entropy.ipynb#ch0000008?line=69'>70</a>\u001b[0m policy_opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/warthog_rl/ppo_warthog_extended_entropy.ipynb#ch0000008?line=70'>71</a>\u001b[0m loss_pi\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akhil/warthog_rl/ppo_warthog_extended_entropy.ipynb#ch0000008?line=71'>72</a>\u001b[0m policy_opt\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/warthog_rl/ppo_warthog_extended_entropy.ipynb#ch0000008?line=72'>73</a>\u001b[0m val \u001b[39m=\u001b[39m vi(o_b)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/warthog_rl/ppo_warthog_extended_entropy.ipynb#ch0000008?line=73'>74</a>\u001b[0m value_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(val\u001b[39m.\u001b[39mflatten(), ret_b)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    142\u001b[0m            grads,\n\u001b[1;32m    143\u001b[0m            exp_avgs,\n\u001b[1;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m            state_steps,\n\u001b[1;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo/lib/python3.8/site-packages/torch/optim/_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     94\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m     96\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m     98\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[1;32m    100\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#python.dataScience.textOutputLimit = 0\n",
    "pi = ppo(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_label(train):\n",
    "    train_obs = train[:,0:42]\n",
    "    train_labels = train[:,42:44]\n",
    "    train_labels[:, 0] = train_labels[:,0]/4\n",
    "    train_labels[:, 1] = train_labels[:,1]/2.5\n",
    "    return train_obs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07231345 0.0165759  0.07507858 0.07209711 0.06828034 0.0619555\n",
      "  0.06238285 0.0615927  0.05430672]\n",
      " [0.02570581 0.00690039 0.01382542 0.01970495 0.01180726 0.01107137\n",
      "  0.006247   0.00913876 0.01050633]\n",
      " [0.0148686  0.00449133 0.00577574 0.00952769 0.00438326 0.00491692\n",
      "  0.00138387 0.00323553 0.00442799]]\n"
     ]
    }
   ],
   "source": [
    "obs_file_name = \"entropy_manual.csv\"\n",
    "df = pd.read_csv(obs_file_name, header=None)\n",
    "model_file_name = \"./temp_policy/tan_h/manaul_entropy3_ppo_batch_527100_rew_2316.pt\"\n",
    "model = torch.load(model_file_name)\n",
    "num_data_points = len(df.index)\n",
    "df = df.to_numpy()\n",
    "msk = np.random.rand(len(df)) < 0.98\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "batch_size = 512\n",
    "num_batches = int(len(train)/batch_size)\n",
    "#num_batches = 1\n",
    "n_epochs = 3\n",
    "train_obs, train_label = get_obs_label(train)\n",
    "test_obs, test_label = get_obs_label(train)\n",
    "opti = optim.Adam(model.parameters(), lr=0.00001)\n",
    "losses = np.zeros([n_epochs, num_batches])\n",
    "for i in (range(n_epochs)):\n",
    "    for k in range(num_batches):\n",
    "        start_idx = k*batch_size\n",
    "        end_idx = (k+1)*batch_size\n",
    "        batch_obs = train_obs[start_idx:end_idx]\n",
    "        batch_label = train_label[start_idx:end_idx]\n",
    "        batch_obs = torch.flatten(torch.as_tensor(batch_obs, dtype=torch.float32), start_dim=1)\n",
    "        batch_label = torch.flatten(torch.as_tensor(batch_label, dtype=torch.float32), start_dim=1)\n",
    "        batch_target = model(batch_obs).loc \n",
    "        loss = F.mse_loss(batch_target, batch_label)\n",
    "        #print(batch_target[0:3])\n",
    "        ##print(batch_label[0:3])\n",
    "      #  print(loss.item())\n",
    "        opti.zero_grad()\n",
    "        loss.backward()\n",
    "        opti.step()\n",
    "        #print(model(batch_obs[0:3]).loc)\n",
    "        losses[i][k] = loss.item()\n",
    "    #losses = np.array(losses)/num_batches\n",
    "#print(losses)\n",
    "print(losses[:,1:10])\n",
    "torch.save(model,'./temp_policy/tan_h/final2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(pi,\"temp_warthog.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "122de4a4be327aa1ba4f735a2434ab4bffab9a9a54192dfc85777065cfde1c01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
