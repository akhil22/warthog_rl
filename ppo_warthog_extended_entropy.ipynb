{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib widget\n",
    "import torch\n",
    "import gym\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import Normal\n",
    "from env.WarthogEnvSup import WarthogEnv\n",
    "import scipy.signal\n",
    "import time\n",
    "import pandas as pd\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "#torch.manual_seed(100)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, act = nn.ReLU):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [1]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.v = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkCat(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension, act= nn.ReLU):\n",
    "        super(PolicyNetworkCat, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.pi = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        score = self.pi(x)\n",
    "        #probs = F.softmax(score,dim = 1)\n",
    "        dist = torch.distributions.Categorical(logits=score)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkGauss(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension,act = nn.ReLU):\n",
    "        super(PolicyNetworkGauss, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.mu = nn.Sequential(*self.layers)\n",
    "        log_std = -0.5*np.ones(action_dimension, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std), requires_grad=True)\n",
    "    def forward(self, x):\n",
    "        mean = self.mu(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "1\n",
      "Normal(loc: torch.Size([1, 2]), scale: torch.Size([1, 2]))\n",
      "tensor([[-0.8280, -0.5553]], grad_fn=<SubBackward0>)\n",
      "tensor([[-0.0486, -0.0167]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "#env = gym.make('CartPole-v1')\n",
    "env = WarthogEnv('sim_remote_waypoint.txt', 'entropy_manual.csv')\n",
    "sizes = [128]\n",
    "obs_dimension = env.observation_space.shape\n",
    "action_dimension = env.action_space.shape\n",
    "pi = PolicyNetworkGauss(*obs_dimension, sizes, *action_dimension)\n",
    "o = torch.zeros(1,42)\n",
    "a = torch.as_tensor([0.5, 0.3], dtype=torch.float32)\n",
    "print(o)\n",
    "m = pi(o)\n",
    "logp = m.log_prob(a)\n",
    "print(len(a.shape))\n",
    "print(m)\n",
    "print(logp)\n",
    "print(m.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        #self.act_buf = np.zeros((size,), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size,act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "       # print(rews)\n",
    "       # print(vals)\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "\n",
    "        self.adv_buf = (self.adv_buf - self.adv_buf.mean()) / (self.adv_buf.std()+eps)\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env, seed = 0, buff_size = 128, batch_size = 4, train_time_steps = 10000000, gamma = 0.99, clip_ratio = 0.2, lr_pi = 3e-5, \n",
    "        lr_vf = 1e-3, pi_train_itrs = 4, v_train_itrs = 4, lam = 0.97, max_ep_len = 500, ent_coeff = 0.01):\n",
    "        obs_dim = env.observation_space.shape\n",
    "        action_dim = env.action_space.shape\n",
    "        h_sizes = [64,64]\n",
    "        vi = ValueNetwork(*obs_dim, h_sizes, act=nn.Tanh).to(device)\n",
    "        pi = PolicyNetworkGauss(*obs_dim, h_sizes, *action_dim,act=nn.Tanh).to(device)\n",
    "        data_buff = PPOBuffer(*obs_dim, *action_dim, buff_size)\n",
    "        policy_opt = optim.Adam(pi.parameters(), lr = lr_pi, eps=1e-5)\n",
    "        value_opt = optim.Adam(vi.parameters(), lr = lr_vf, eps=1e-5)\n",
    "        obs = env.reset()\n",
    "        curr_time_step = 0\n",
    "        num_episode = 0\n",
    "        ep_rewards = [0]\n",
    "        ep_steps = [0]\n",
    "        start_time = time.time()\n",
    "        while curr_time_step < train_time_steps: \n",
    "                for t in range(0, buff_size):\n",
    "                        curr_time_step+=1\n",
    "                        with torch.no_grad():\n",
    "                                m = pi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                action = m.sample()\n",
    "                                action = action.reshape((-1,) + action_dim)\n",
    "                                logp = m.log_prob(action).sum(dim=1)\n",
    "                                action = action.cpu().numpy() \n",
    "                                clipped_action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "                                obs_new, rew, done, _ = env.step(clipped_action[0])\n",
    "                                ep_rewards[num_episode]+=rew \n",
    "                                ep_steps[num_episode]+=1\n",
    "                                v = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                        data_buff.store(obs, action, rew, v.cpu().numpy(), logp.cpu().numpy())\n",
    "                        obs = obs_new\n",
    "                        if done or buff_size-1:\n",
    "                                if done:\n",
    "                                        v_ = 0.\n",
    "                                        obs = env.reset()\n",
    "                                        done = False\n",
    "                                        num_episode+=1\n",
    "                                        ep_rewards.append(0)\n",
    "                                        ep_steps.append(0)\n",
    "                                        curr_time = time.time()\n",
    "                                        if num_episode %100 == 0:\n",
    "                                                print(f'episode: {num_episode-1} \\t episode_reward: {np.mean(ep_rewards[-10:-2])} \\t total steps:{curr_time_step}\\t fps\"{curr_time_step/(curr_time-start_time)}\\t avg_ep_steps: {np.mean(ep_steps[-10:-2])}')\n",
    "                                                torch.save(pi, f'./temp_policy/tan_h/manaul_entropy3_ppo_batch_{curr_time_step}_rew_{int(np.mean(ep_rewards[-10:-2]))}.pt')\n",
    "                                else:\n",
    "                                        v_ = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                        v_ = v_.detach().cpu().numpy()\n",
    "                                data_buff.finish_path(v_)\n",
    "                        if(curr_time_step % 100000 == 0):\n",
    "                                np.savetxt(f'./temp_policy/tan_h/avg_rew_{curr_time_step}', ep_rewards, fmt='%f')\n",
    "\n",
    "                data = data_buff.get()\n",
    "                ret, act, adv, o, logp_old= data['ret'].to(device), data['act'].to(device), data['adv'].to(device), data['obs'].to(device), data['logp'].to(device)\n",
    "                for j in range(0, pi_train_itrs):\n",
    "                        indices = np.random.permutation(buff_size)\n",
    "                        start_idx = 0\n",
    "                        while start_idx < buff_size:\n",
    "\n",
    "                                batch_ind = indices[start_idx: start_idx+batch_size]\n",
    "                                ret_b = ret[batch_ind]\n",
    "                                act_b = act[batch_ind]\n",
    "                                adv_b = adv[batch_ind]\n",
    "                                o_b = o[batch_ind]\n",
    "                                logp_old_b = logp_old[batch_ind]\n",
    "                                act_dist = pi(o_b)\n",
    "                                logp = act_dist.log_prob(act_b).sum(dim=1)\n",
    "                                ratio = torch.exp(logp - logp_old_b)\n",
    "                                clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv_b\n",
    "                                loss_pi = -(torch.min(ratio * adv_b, clip_adv)).mean() - ent_coeff* torch.mean(-logp)\n",
    "                                policy_opt.zero_grad()\n",
    "                                loss_pi.backward()\n",
    "                                policy_opt.step()\n",
    "                                val = vi(o_b)\n",
    "                                value_loss = F.mse_loss(val.flatten(), ret_b)\n",
    "                                value_opt.zero_grad()\n",
    "                                value_loss.backward()\n",
    "                                value_opt.step()\n",
    "                                start_idx = start_idx+batch_size\n",
    "        return pi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 99 \t episode_reward: 92.23993190940232 \t total steps:18200\t fps\"438.64286846407754\t avg_ep_steps: 262.5\n",
      "episode: 199 \t episode_reward: 399.3429495916526 \t total steps:49419\t fps\"438.1008062198512\t avg_ep_steps: 388.875\n",
      "episode: 299 \t episode_reward: 653.4863804789726 \t total steps:91185\t fps\"430.52352451475434\t avg_ep_steps: 444.0\n",
      "episode: 399 \t episode_reward: 764.0229971544452 \t total steps:121100\t fps\"410.6935359886321\t avg_ep_steps: 423.75\n",
      "episode: 499 \t episode_reward: 1784.2280635655688 \t total steps:163124\t fps\"403.70672397737314\t avg_ep_steps: 594.75\n",
      "episode: 599 \t episode_reward: 2342.927661235311 \t total steps:210205\t fps\"402.04960058048414\t avg_ep_steps: 700.0\n",
      "episode: 699 \t episode_reward: 2879.2451460523876 \t total steps:255500\t fps\"397.2401770425665\t avg_ep_steps: 700.0\n",
      "episode: 799 \t episode_reward: 1617.586448950226 \t total steps:286135\t fps\"394.38876286097417\t avg_ep_steps: 402.0\n",
      "episode: 899 \t episode_reward: 228.90108535627556 \t total steps:329910\t fps\"393.2367847694837\t avg_ep_steps: 112.25\n",
      "episode: 999 \t episode_reward: 1044.8404310324177 \t total steps:380100\t fps\"392.8679706640058\t avg_ep_steps: 350.0\n",
      "episode: 1099 \t episode_reward: 2038.684615895725 \t total steps:429100\t fps\"392.25266100737895\t avg_ep_steps: 437.5\n",
      "episode: 1199 \t episode_reward: 1776.734374823312 \t total steps:483920\t fps\"391.983296541238\t avg_ep_steps: 454.0\n",
      "episode: 1299 \t episode_reward: 303.75325779451 \t total steps:519761\t fps\"391.6706683425025\t avg_ep_steps: 114.75\n",
      "episode: 1399 \t episode_reward: 1465.211313393854 \t total steps:546149\t fps\"391.08750172563225\t avg_ep_steps: 612.5\n",
      "episode: 1499 \t episode_reward: 176.4175045315161 \t total steps:595148\t fps\"389.60153901039655\t avg_ep_steps: 72.125\n",
      "episode: 1599 \t episode_reward: 692.0179038304259 \t total steps:629300\t fps\"388.40847535005673\t avg_ep_steps: 309.25\n",
      "episode: 1699 \t episode_reward: 187.22059348569636 \t total steps:659629\t fps\"387.65396622809374\t avg_ep_steps: 67.0\n",
      "episode: 1799 \t episode_reward: 290.54819329543614 \t total steps:669315\t fps\"387.62329503786236\t avg_ep_steps: 88.25\n",
      "episode: 1899 \t episode_reward: 122.1690401935646 \t total steps:677522\t fps\"387.63104514293855\t avg_ep_steps: 64.375\n",
      "episode: 1999 \t episode_reward: 100.43814960801285 \t total steps:682621\t fps\"387.68180454020165\t avg_ep_steps: 44.5\n",
      "episode: 2099 \t episode_reward: 94.50010945959896 \t total steps:687990\t fps\"387.6833113327387\t avg_ep_steps: 58.375\n",
      "episode: 2199 \t episode_reward: 125.75692014549367 \t total steps:693510\t fps\"387.63122961599566\t avg_ep_steps: 55.0\n",
      "episode: 2299 \t episode_reward: 147.2524520472332 \t total steps:698775\t fps\"387.6399833092153\t avg_ep_steps: 55.125\n",
      "episode: 2399 \t episode_reward: 119.295780036621 \t total steps:704900\t fps\"387.6160453262353\t avg_ep_steps: 55.25\n",
      "episode: 2499 \t episode_reward: 161.28887481055472 \t total steps:710500\t fps\"387.6430712776533\t avg_ep_steps: 64.5\n",
      "episode: 2599 \t episode_reward: 136.4188037515166 \t total steps:716348\t fps\"387.642312439064\t avg_ep_steps: 52.625\n",
      "episode: 2699 \t episode_reward: 123.18147212607381 \t total steps:722067\t fps\"387.6183047747149\t avg_ep_steps: 48.625\n",
      "episode: 2799 \t episode_reward: 240.86303773664224 \t total steps:730091\t fps\"387.6220384998497\t avg_ep_steps: 85.625\n",
      "episode: 2899 \t episode_reward: 190.79323460162516 \t total steps:736644\t fps\"387.5634348294411\t avg_ep_steps: 64.25\n",
      "episode: 2999 \t episode_reward: 992.832032811704 \t total steps:746593\t fps\"387.5160957216569\t avg_ep_steps: 201.25\n",
      "episode: 3099 \t episode_reward: 588.838286874789 \t total steps:793100\t fps\"386.6585989088512\t avg_ep_steps: 194.5\n",
      "episode: 3199 \t episode_reward: 1975.885445619197 \t total steps:831829\t fps\"386.13343845811886\t avg_ep_steps: 396.875\n",
      "episode: 3299 \t episode_reward: 319.1624159164702 \t total steps:859074\t fps\"385.5881484175538\t avg_ep_steps: 131.25\n",
      "episode: 3399 \t episode_reward: 732.2390961600737 \t total steps:879511\t fps\"384.713988310177\t avg_ep_steps: 177.25\n",
      "episode: 3499 \t episode_reward: 710.2067857624972 \t total steps:889624\t fps\"384.27315756888765\t avg_ep_steps: 221.0\n",
      "episode: 3599 \t episode_reward: 1340.1398869446177 \t total steps:911410\t fps\"383.24268716725754\t avg_ep_steps: 391.75\n",
      "episode: 3699 \t episode_reward: 1063.6765998120086 \t total steps:932456\t fps\"382.5206163723316\t avg_ep_steps: 255.375\n",
      "episode: 3799 \t episode_reward: 652.783284424466 \t total steps:950244\t fps\"381.98542710067613\t avg_ep_steps: 184.625\n",
      "episode: 3899 \t episode_reward: 2464.3102763070133 \t total steps:996100\t fps\"380.52182527515276\t avg_ep_steps: 700.0\n",
      "episode: 3999 \t episode_reward: 497.80617726185176 \t total steps:1020151\t fps\"380.3311122357057\t avg_ep_steps: 136.875\n",
      "episode: 4099 \t episode_reward: 1300.5139299230902 \t total steps:1052212\t fps\"379.8606019083679\t avg_ep_steps: 318.875\n",
      "episode: 4199 \t episode_reward: 255.54853995694893 \t total steps:1069029\t fps\"379.71015848034045\t avg_ep_steps: 89.125\n",
      "episode: 4299 \t episode_reward: 733.1781072120143 \t total steps:1077801\t fps\"379.6374632705151\t avg_ep_steps: 186.25\n",
      "episode: 4399 \t episode_reward: 2748.7723214932857 \t total steps:1108100\t fps\"379.2958053912956\t avg_ep_steps: 700.0\n",
      "episode: 4499 \t episode_reward: 1792.1930892478001 \t total steps:1149400\t fps\"378.88690282197143\t avg_ep_steps: 612.5\n",
      "episode: 4599 \t episode_reward: 361.84913312696415 \t total steps:1187200\t fps\"378.5448641984799\t avg_ep_steps: 218.25\n",
      "episode: 4699 \t episode_reward: 197.79402791772912 \t total steps:1211755\t fps\"378.4599316171007\t avg_ep_steps: 83.375\n",
      "episode: 4799 \t episode_reward: 152.00489319758947 \t total steps:1218519\t fps\"378.4555297827438\t avg_ep_steps: 61.0\n",
      "episode: 4899 \t episode_reward: 356.5759043741111 \t total steps:1227185\t fps\"378.45480438482394\t avg_ep_steps: 103.875\n",
      "episode: 4999 \t episode_reward: 680.7706694938993 \t total steps:1240598\t fps\"378.3741712148429\t avg_ep_steps: 161.0\n",
      "episode: 5099 \t episode_reward: 646.5186756164613 \t total steps:1260527\t fps\"378.1769292042403\t avg_ep_steps: 216.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#python.dataScience.textOutputLimit = 0\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pi \u001b[39m=\u001b[39m ppo(env)\n",
      "Cell \u001b[1;32mIn[8], line 65\u001b[0m, in \u001b[0;36mppo\u001b[1;34m(env, seed, buff_size, batch_size, train_time_steps, gamma, clip_ratio, lr_pi, lr_vf, pi_train_itrs, v_train_itrs, lam, max_ep_len, ent_coeff)\u001b[0m\n\u001b[0;32m     63\u001b[0m o_b \u001b[39m=\u001b[39m o[batch_ind]\n\u001b[0;32m     64\u001b[0m logp_old_b \u001b[39m=\u001b[39m logp_old[batch_ind]\n\u001b[1;32m---> 65\u001b[0m act_dist \u001b[39m=\u001b[39m pi(o_b)\n\u001b[0;32m     66\u001b[0m logp \u001b[39m=\u001b[39m act_dist\u001b[39m.\u001b[39mlog_prob(act_b)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m ratio \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(logp \u001b[39m-\u001b[39m logp_old_b)\n",
      "File \u001b[1;32mc:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mPolicyNetworkGauss.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmu(x)\n\u001b[0;32m     15\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_std)\n\u001b[1;32m---> 16\u001b[0m dist \u001b[39m=\u001b[39m Normal(mean, std)\n\u001b[0;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m dist\n",
      "File \u001b[1;32mc:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\torch\\distributions\\normal.py:51\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, loc, scale, validate_args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale \u001b[39m=\u001b[39m broadcast_all(loc, scale)\n\u001b[0;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, Number) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(scale, Number):\n\u001b[0;32m     53\u001b[0m         batch_shape \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mSize()\n",
      "File \u001b[1;32mc:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\torch\\distributions\\utils.py:42\u001b[0m, in \u001b[0;36mbroadcast_all\u001b[1;34m(*values)\u001b[0m\n\u001b[0;32m     39\u001b[0m     new_values \u001b[39m=\u001b[39m [v \u001b[39mif\u001b[39;00m is_tensor_like(v) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(v, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m     40\u001b[0m                   \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m values]\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39m*\u001b[39mnew_values)\n\u001b[1;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39m*\u001b[39;49mvalues)\n",
      "File \u001b[1;32mc:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#python.dataScience.textOutputLimit = 0\n",
    "pi = ppo(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_label(train):\n",
    "    train_obs = train[:,0:42]\n",
    "    train_labels = train[:,42:44]\n",
    "    train_labels[:, 0] = train_labels[:,0]/4\n",
    "    train_labels[:, 1] = train_labels[:,1]/2.5\n",
    "    return train_obs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akhil\\AppData\\Local\\Temp\\ipykernel_17984\\1695251876.py:2: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(obs_file_name, header=None)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m batch_obs \u001b[39m=\u001b[39m train_obs[start_idx:end_idx]\n\u001b[0;32m     31\u001b[0m batch_label \u001b[39m=\u001b[39m train_label[start_idx:end_idx]\n\u001b[1;32m---> 32\u001b[0m batch_obs_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(torch\u001b[39m.\u001b[39;49mas_tensor(batch_obs, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32), start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m batch_label_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(torch\u001b[39m.\u001b[39mas_tensor(batch_label, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32), start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m batch_target_tensor \u001b[39m=\u001b[39m model(batch_obs_tensor)\u001b[39m.\u001b[39mloc \n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "obs_file_name = \"./entropy_manual.csv\"\n",
    "df = pd.read_csv(obs_file_name, header=None)\n",
    "model_file_name = \"./temp_policy/tan_h/manaul_entropy3_ppo_batch_210205_rew_2342.pt\"\n",
    "model = torch.load(model_file_name)\n",
    "obs_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.shape\n",
    "h_sizes = [64,64]\n",
    "#model = PolicyNetworkGauss(*obs_dim, h_sizes, *action_dim,act=nn.Tanh).to(device)\n",
    "num_data_points = len(df.index)\n",
    "df = df.to_numpy()\n",
    "num_obs = 500000\n",
    "df = df[0:num_obs]\n",
    "msk = np.random.rand(len(df)) < 0.98\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "#batch_size = 512 best batch size\n",
    "batch_size = 512\n",
    "num_batches = int(len(train)/batch_size)\n",
    "#num_batches = 1\n",
    "n_epochs = 10\n",
    "train_obs, train_label = get_obs_label(train)\n",
    "test_obs, test_label = get_obs_label(test)\n",
    "#opti = optim.Adam(model.parameters(), lr=0.0001)\n",
    "opti = optim.SGD(model.parameters(), lr=0.1)\n",
    "losses = np.zeros(n_epochs)\n",
    "for i in (range(n_epochs)):\n",
    "    for k in range(num_batches):\n",
    "        start_idx = k*batch_size\n",
    "        end_idx = min((k+1)*batch_size, len(train))\n",
    "        batch_obs = train_obs[start_idx:end_idx]\n",
    "        batch_label = train_label[start_idx:end_idx]\n",
    "        batch_obs_tensor = torch.flatten(torch.as_tensor(batch_obs, dtype=torch.float32), start_dim=1)\n",
    "        batch_label_tensor = torch.flatten(torch.as_tensor(batch_label, dtype=torch.float32), start_dim=1)\n",
    "        batch_target_tensor = model(batch_obs_tensor).loc \n",
    "        loss = F.mse_loss(batch_target_tensor, batch_label_tensor)\n",
    "        losses[i] = losses[i] + loss.item()\n",
    "        #print(batch_target[0:3])\n",
    "        ##print(batch_label[0:3])\n",
    "      #  print(loss.item())\n",
    "        opti.zero_grad()\n",
    "        loss.backward()\n",
    "        opti.step()\n",
    "        #print(model(batch_obs[0:3]).loc)\n",
    "        #losses[i][k] = loss.item()\n",
    "    #losses = np.array(losses)/num_batches\n",
    "#print(losses)\n",
    "print(losses)\n",
    "torch.save(model,f'./temp_policy/tan_h/check_meet_{num_obs}_{n_epochs}_{batch_size}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(pi,\"temp_warthog.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4740f7e2305a459a935d8d587c8fcbc1b1d6d7caacdc0daf6a3c40a8f96c94af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
