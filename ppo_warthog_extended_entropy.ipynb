{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\rl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib widget\n",
    "import torch\n",
    "import gym\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import Normal\n",
    "from env.WarthogEnvSup import WarthogEnv\n",
    "import scipy.signal\n",
    "import time\n",
    "import pandas as pd\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "#torch.manual_seed(100)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, act = nn.ReLU):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [1]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.v = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkCat(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension, act= nn.ReLU):\n",
    "        super(PolicyNetworkCat, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.pi = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        score = self.pi(x)\n",
    "        #probs = F.softmax(score,dim = 1)\n",
    "        dist = torch.distributions.Categorical(logits=score)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkGauss(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension,act = nn.ReLU):\n",
    "        super(PolicyNetworkGauss, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.mu = nn.Sequential(*self.layers)\n",
    "        log_std = -0.5*np.ones(action_dimension, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std), requires_grad=True)\n",
    "    def forward(self, x):\n",
    "        mean = self.mu(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\rl\\lib\\site-packages\\gym\\spaces\\box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "1\n",
      "Normal(loc: torch.Size([1, 2]), scale: torch.Size([1, 2]))\n",
      "tensor([[-0.8761, -0.5498]], grad_fn=<SubBackward0>)\n",
      "tensor([[-0.0800, -0.0103]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz0AAAMkCAYAAACSsU4mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAut0lEQVR4nO3de5Tdd13v/9enTRtImxloUtopbSkXgYJASVuEHqmIi3LTKBcFj7qo/gw5uMTlz5+eYw9oEkBQXOIFUHMCCnKRI2cBFfmhiFAQilQ6losWkFpoY6fQtGWmNCFt08/5Y0+SSZo0mTR7dvKex2Otvfbe39nf7/cz2bnMM5/v97tb7z0AAABVHTPqAQAAAAyT6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEpbsOhprV3SWuuttT9YqH0CAAAsSPS01s5P8tIkX1iI/QEAAOw09OhprZ2Y5F1J1iS5ddj7AwAAmGvJAuzjzUk+1Hv/aGvtlff2wtba0iRL91p8UpJbhjU4AADgqLE8yQ299z6flYYaPa21Fyc5N8l5B7nKJUnWDW9EAADAUe70JP85nxWGFj2ttTOS/GGSi3rv3z3I1V6X5A1zni9Psvn666/P2NjY4R4iAABwlJiZmckZZ5yRJLfNd91hzvScm+RBSa5sre1cdmySC1trv5hkae99x9wVeu/bk2zf+XznemNjY6IHAAA4JMOMnn9I8ri9lv15ki8n+Z29gwcAAGAYhhY9vffbknxp7rLW2u1Jbu69f2nfawEAABxeC/bhpAAAAKOwEJes3qX3/rSF3B8AAICZHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAABabqalk/frB/SIgegAAYLGZmko2bBA9AAAAFSwZ9QAAAIAFMDW1e2ZncnLP+ySZmBjcChI9AACwGGzcODikba41a3Y/XrducJ5PQaIHAAAWg7Vrk9WrB48nJwfBs2lTsmrVYFnRWZ5E9AAAwOKwr8PXVq3aHT2FuZABAABQmugBAIDFZmJicA5P4UPa5mq991GPYb9aa2NJpqenpzM2Njbq4QAAACMyMzOT8fHxJBnvvc/MZ10zPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAOPymppL16wf3IyZ6AACAw29qKtmwQfQAAAAM25JRDwAAAChiamr3zM7k5J73STIxMbgtMNEDAAAcHhs3Dg5pm2vNmt2P160bnOezwEQPAABweKxdm6xePXg8OTkInk2bklWrBstGMMuTiB4AAOBw2dfha6tW7Y6eEXEhAwAAoDTRAwAAHH4TE4NzeEZ0SNtcrfc+6jHsV2ttLMn09PR0xsbGRj0cAABgRGZmZjI+Pp4k4733mfmsa6YHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaUONntbay1prX2itzczePtNae/Yw9wkAADDXsGd6Nif59STnzd4+luTS1tpjh7xfAACAJMmSYW689/7BvRa9orX2siRPTvKvw9w3AABAMuTomau1dmySH09yQpLPLNR+AQCAxW3o0dNae1wGkXO/JN9J8rze+7/t57VLkyyds2j5sMcHAADUthBXb/tKknMyOKTtT5K8vbX2mP289pIk03NumxdgfAAAQGGt976wO2zto0mu6b2v3cfX9jXTs3l6ejpjY2MLNUQAAOAIMzMzk/Hx8SQZ773PzGfdBTunZ46WPcNml9779iTbd72wtYUaEwAAUNRQo6e19tokH05yfQazNi9O8rQkzxrmfgEAAHYa9kzPKUnekWQig3N0vpDkWb33vx/yfgEAAJIM/3N6/p9hbh8AAOBAFuLqbQAAACMjegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAACliR4AAKC0JaMeAAAAwL2amkr+6I8OeXUzPQAAwJFtair57d8+5NVFDwAAUJrD2wAAgCPP1NTgliSTk/dpU6IHAAA48mzcmGzYcFg21Xrvh2VDw9BaG0syPT09nbGxsVEPBwAAWCh7zfTMrFmT8cGz8d77zHw2ZaYHAAA48kxMDG6HgQsZAAAApYkeAADgyDYxkfz6rx/y6s7pAQAAjngzMzMZHx9PDuGcHjM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAAClLRn1AACAhTU9PZ2tW7eOehgssGXLlmV8fHzUw4CRED0AsIhMT0/nTa9+de7csmXUQ2GBHbdyZX7xN35D+LAoiR4AWES2bt2aO7dsyfPvf/+cvGzZqIfDArlp69a8b8uWbN26VfSwKIkeAFiETl62LBPLl496GCykbdtGPQIYGRcyAAAAShM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAobcmoBwAALG4rXv/6/PTjHpc/fPazRz2UI8ZbJifzxiuuyFdvvjknL1uWXzj//Pz693//qIcFRy3RAwCMzDW33JJbtm3L+Q9+8KiHcsRY+8EP5u2f/3xeeeGFueCMM/KBL385l/zDP2TixBPzknPOGfXw4KgkegCAkfncDTckSc4/7bQF2d+dO3ZkyTHHpLV2j69tv+uuLF1y3340urftH4y3XXVV/tfkZP7yBS/Ii7/3e5MkT3/oQ/Oxa6/NG6+4QvTAIRI9AMBBe+67350vb9mSa37pl/ZY3nvPk97ylhx/7LH59M/9XJLkr7/ylfzOpz+dz994Y5Ydd1x+6GEPy+8/85k59cQTd633zzfckLGlS/PIFSvmNY4vfvOb+c3LLssnv/GN3LFjR1ZNTOT3Lroo582Jp3d/8Yv5qfe9L5e95CX5s6uuyoe++tXcfuedue2SS/L6T30q6y67LFf8/M/nVZ/8ZD527bV50Akn5N9f/vIkyd9fc01+9/LL88833JCW5ClnnJHfu+iiPHrlyoPa/pJDiJ47duzIKz72sTztrLN2Bc9OTzj11PzNV786720CAy5kAAActHNOOSXX3nprvnPHHXss/4vPfz5X3nBD/vBZz0rvPT936aV54V/9VZ784AfnfS96UX7vooty+fXX54ff/e7suPvuXet97oYbcu7ExLxmRt46OZknbtw4eLx6df7yBS9Ikjz97W/P5pmZXa+bnJpKS3LxpZfm9OXL854XvjDv+4mfyJJjjsnk1FTut2RJVr/nPXnygx+c97/oRflfP/zDSZLf/fSn86x3vSsPf+AD81cvfGHeunp1pm67LU9729ty0+23H9T2d9x9d+46iFvvfdf2PvTVr+aG227Ly8477x7f84677862O+886F8jYE9megCAg/aEU09NT/Kv3/pWvu/005Mkt99xR/7nxz6Wl5xzTs477bS86Yor8udXXZW/euEL8+OPfeyudU898cRc9M535p82b85/OfPM9N4zOTW1zx/y9+fzN96YtX/zN3npuefmj5/73F3Ln3rmmTntDW/I26+6Kq+48MIkyZVTU0mSNz/nOXnO93zPHtu5cmoqd+7YkQ/+5E/mnFNP3bX8Y9dem//+0Y/m95/5zPzyk5+8a/mjVq7MY//4j/Pef/u3/ML55x9w+0tf85rsmBM0+/O7z3hGfvWCC5Ik//+//3uOaS3PfsQj7vG6b95+e04+4YQDbg/YN9EDABy0J5xySpLki3Oi57c/9anMbN+e1z796dlx99151Sc+kaeddVaed/bZuWvOrM5jH/SgJMl/3Hpr/suZZ+bLW7bktjvu2OOQtAPZ8IlP5MTjj89vPf3pe2z7hOOPz5nj4/mPW29NMjjc7l+mpvLcRz7yHkFyy7Zt+fq3v51fPP/8PYJn5/fy8Ac+MC9/0pP2WP49J52UJLluevqA20+Sz/78z+fAyZOcMTa26/HkjTfmkStWZPnSpXu8Zue+nvqQhxzEFoF9ET0AwEH7nhUrsuy44/LFb34zSXL99HR+7zOfyW9ceGEmli/PP23enJu2bs1lX/96jnv1q/e5jQfc735J5lzE4CCv3HZ37/nbr30t2+66Kye9/vX7fM1zZmdJrrn11kxv357nP/rR93jN5OwMzfPPPnuP5Xfu2JFPfOMb+W/nnptjj9nzDICdsXP6bKTc2/aT5JzZGbEDOXbOYX3fuv32PGI2rub69PXXZ3r79n3OAAEHZ6jR01q7JMnzkzw6ybYklyf5H733rwxzvwDAcBzTWr73QQ/KF7/1rSTJ//joR3PqiSfmV57ylCSDCEqS97zgBXn4Pn6AT5LHnHxykkH0rFy2LGc94AEHte+bt27Ntrvuyq9dcEF+Ys5hc3PtnDm5cjaods5GzXXlDTfkmNbuEVvT27fnjh07MrF8+T3W+duvfS1J8oyHPeyA208O7fC2+y9ZskcE7bRpcjInHHfcfr9n4MCGPdPzA0nenOSfZ/f1W0k+0lp7TO/99ntdEwA4Ij3hlFPy/i9/Of+0eXPe86Uv5b0//uO7LvV80v3vnyS535IlBzxs7Z9vuGFeh7YtX7o0xx1zTO7cseOA6105NZUTjz9+j6utzf3ao1asyInHH7/H8pXLluXE44/PV26+eY/l37r99rz6k5/MjzzykXnU7PbubfvJoR3e9sgVKzI5NZW77r47S2Znmi6//vq88wtfyG9eeGFWLlt2EFsE9mWo0dN7f9bc5621n03yrSTnJvnkMPcNAAzHE045JZsmJ/Nzl16aCx/ykLzgMY/Z9bWnPuQhedSKFXnp3/xNrv32t/OEU07J9h078p8zM/m7a67Ja57+9DxyxYrsuPvuXHXjjbtmOQ7G/ZYsyU8//vF54xVXZOmSJfnBs87Kcccemxu/8518+rrr8tSHPGTXpZ4np6byxFNPzTH7mDmZnJrKBWecsc99vHTVqrzxiivyyJNOylPOOCNfu+WWvPYf/zErli3Ln/3oj+6xjf1tP0nOPYTPHXr5k56UZ73rXfn5v/7r/MzjH59/vemmrLvssjzz4Q/PK2cvzgAcmoU+p2d89v6WfX2xtbY0ydyz9+45vwwAjNQTZk/+/8rNN+fds5eL3un4Y4/NZRdfnFd/4hP5o89+NjfcdluWL12ahz7gAXn6Qx+ahz/wgUmSf73ppmy76655zfQkyZ8897l59MqV+YvPfz5vuuKKHHvMMXnw8uX5/jPP3CNkJqemcvE+Pshz+rvfzX/cemt+6fu+b5/bf+0P/VBOOP74vOVf/iUbPvGJnD42lhc99rF5xYUX7jEztL/t3xfPfMQj8ubnPCev//Sn85df+lIe9sAH5n9+//fnV57ylHucYwTMT+sHcbzpYdnR4AL8lyZ5YO/9qft5zfok6/ZePj09nbE5078AwKGZmprKxksuydoVK/Z57go1Td12WzbefHPWvu51mZiYGPVw4JDMzMxkfHw8ScZ77zMHev1cC/nfBm9K8vgkP3kvr3ldBrNBO2/7PjsQAADgIC3I4W2ttTcmWZ3kwt775v29rve+Pcn2OestwOgAgFHrvR/wamfHtuZnA+CQDPuS1S3JG5M8L8nTeu/XDnN/AMDRaeOVV+ZlH/rQvb7m4y95SZ521lkLMyCglGHP9Lw5yX9N8qNJbmut7fzY4+ne+7Yh7xsAOEq84OyzD3hRg0etWLFAowGqGXb0vGz2/rK9lv9skrcNed8AwFHi5BNOyMknnDDqYQBFDftzehx4CwAAjJSLvgMAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoLQlox4AALDwbtq6ddRDYAF5v1nsRA8ALCLLli3LcStX5n1btiTbto16OCyg41auzLJly0Y9DBiJ1nsf9Rj2q7U2lmR6eno6Y2Njox4OAJQwPT2drf7nf9FZtmxZxsfHRz0MOGQzMzM7fw+P995n5rOumR4AWGTGx8f98AssKi5kAAAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABwZJiaStavH9wfRqIHAAA4MkxNJRs2iB4AAID5WDLqAQAAAIvY1NTumZ3JyT3vk2RiYnC7D0QPAAAwOhs3Dg5pm2vNmt2P160bnOdzH4geAABgdNauTVavHjyenBwEz6ZNyapVg2X3cZYnET0AAMAo7evwtVWrdkfPYeBCBgAAQGmiBwAAODJMTAzO4TkMh7TN1Xrvh3WDh1NrbSzJ9PT0dMbGxkY9HAAAYERmZmYyPj6eJOO995n5rGumBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAOLCpqWT9+sH9UWao0dNau7C19sHW2g2ttd5a+7Fh7g8AABiSqalkwwbRsw8nJPl8kl8c8n4AAAD2ackwN957/3CSDydJa22YuwIAAA63qandMzuTk3veJ8nExOB2hBtq9MxXa21pkqVzFi0f1VgAAGDR27hxcEjbXGvW7H68bt3gPJ8j3BEVPUkuSbJu1IMAAACSrF2brF49eDw5OQieTZuSVasGy46CWZ7kyIue1yV5w5zny5NsHtFYAABgcdvX4WurVu2OnqPEERU9vfftSbbvfO48IAAA4L7yOT0AAMCBTUwMzuE5Sg5pm2uoMz2ttROTPGLOooe21s5Jckvv/bph7hsAADiMJiaOiosW7MuwD287L8nH5zzfeb7O25NcPOR9AwAADP1zei5L4sQcAABgZJzTAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAOJJMTSXr1w/uOSxEDwAAHEmmppING0TPYSR6AACA0paMegAAALDoTU3tntmZnNzzPkkmJgY3DonoAQCAUdu4cXBI21xr1ux+vG7d4DwfDonoAQCAUVu7Nlm9evB4cnIQPJs2JatWDZaZ5blPRA8AAIzavg5fW7Vqd/Rwn7iQAQAAUJroAQCAI8nExOAcHoe0HTat9z7qMexXa20syfT09HTGxsZGPRwARui6667Lli1bRj2MRW3lypU588wzRz0MYJGamZnJ+Ph4koz33mfms65zegA44l133XU5++yzs3Xr1lEPZVFbtmxZrr76auEDHHVEDwBHvC1btmTr1q155zvfmbPPPnvUw1mUrr766vz0T/90tmzZInqAo47oAeCocfbZZ2eVKxkBME8uZAAAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAtUxNJevXD+4hogcAgGqmppING0QPu4geAACgtCWjHgAAANxnU1O7Z3YmJ/e8T5KJicGNRUn0AABw9Nu4cXBI21xr1ux+vG7d4DwfFiXRAwDA0W/t2mT16sHjyclB8GzalKxaNVhmlmdREz0AABz99nX42qpVu6OHRc2FDAAAgNJEDwAAtUxMDM7hcUgbsxzeBgBALRMTLlrAHsz0AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwBgsZqaGnyezdTUqEcCQyV6AAAWq6mpZMMG0UN5ogcAAChtyagHAADAApqa2j2zMzm5532STEwMblDIgsz0tNZ+obV2bWvtu621K1trT12I/QIAsJeNG5Nzzx3c1qwZLFuzZveyjRtHOz4YgqFHT2vtRUn+IMlvJXlikn9M8uHW2pnD3jcANZz7wXOTdbP3R6tKJ4xX+l72p/L3uHZtcuWVg9umTYNlmzbtXrZ27WjHB0OwEDM9v5Lkrb33t/Ter+69/3KS65O8bAH2DUAVbdQDuI8qnTBe6XvZn8rf48REsmrV7luy53OHtlHQUKOntXZ8knOTfGSvL30kyQXD3DcAAEAy/AsZrExybJJv7rX8m0lO3fvFrbWlSZbOWbR8eEMD4EjWNsyZ2rk7g/+mu3vP5Vf+yJULPq75WHLTTTluy5YkybIvfzkPSfKN978/W6++Okly58qVuevkk0c4woN37eWX54lJ7n/11cm2bYOF1U5+X4wn+E9MJOvW1fu+YC+t9z68jbd2WpL/THJB7/0zc5a/IsnP9N4fvdfr1ydZt/d2pqenMzY2NrRxAnDkaevbvR/S1pNsWKjRHJp1Sdbfy9fX54j/FnY50PeSdesG58AczdavHxzStj8Vvkc4is3MzGR8fDxJxnvvM/NZd9gzPVuS7Mg9Z3UelHvO/iTJ65K8Yc7z5Uk2D2doABzR9hU8O2d8Zr9+5ZVH/kzP1XNnel7zmnzjla/M1kcP/s/v+StXZvVRMtOz5KabMnXXXZmYmBjMfqxZMzj5fec5IRVmCtauTVavHjyu+j3CIjXU6Om939FauzLJM5K8f86XnpHk0n28fnuS7Tuft3a0n7UKwKHq63YfibDrkLZj9lx+VJmcTF7zmjzkec/b/UP00W7uifAV7OvwtWrfIyxSC/HhpG9I8o7W2ueSfCbJS5OcmeRPF2DfAADAIjf06Om9/+/W2ookv5lkIsmXkjyn9/6NYe8bgEJ6ju7LVlc6YbzS97I/i+F7hEVkqBcyuK9aa2NJpl3IAAAAFrf7ciGDhfhwUgAAgJERPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApQ01elprr2itXd5a29pa+/Yw9wUAALAvw57pOT7Je5P8yZD3AwAAsE9Lhrnx3vu6JGmtXTzM/QAAAOzPUKNnvlprS5MsnbNo+ajGAgAA1HCkXcjgkiTTc26bRzscAADgaDfv6GmtrW+t9QPczjvE8bwuyfic2+mHuB0AAIAkh3Z425uSvOcAr/n6IWw3vfftSbbvfN5aO5TNAAAA7DLv6Om9b0myZQhjAQAAOOyGeiGD1tqZSU5KcmaSY1tr58x+6Wu99+8Mc98AAADJ8K/e9qokL5nz/F9m738wyWVD3jcAAMBwr97We7+49972cbtsmPsFAADY6Ui7ZDUAAMBhJXoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFCa6AEAAEoTPQAAQGmiBwAAKE30AAAApYkeAACgNNEDAACUJnoAAIDSRA8AAFDa0KKntXZWa+2trbVrW2vbWmvXtNY2tNaOH9Y+AQAA9rZkiNt+dAZRtTbJ15J8b5JNSU5I8qtD3C8AAMAuQ4ue3vvfJvnbOYv+o7X2qCQvi+gBAAAWyEKf0zOe5JYF3icAALCIDfPwtj201h6e5OVJ/r97ec3SJEvnLFo+7HEBAAC1zXump7W2vrXWD3A7b691TsvgULf39t7fci+bvyTJ9Jzb5vmODwAAYK7We5/fCq2tTLLyAC/7eu/9u7OvPy3Jx5N8NsnFvfe772Xb+5rp2Tw9PZ2xsbF5jRMAAKhjZmYm4+PjSTLee5+Zz7rzPryt974lyZaDeW1r7cEZBM+VSX723oJndtvbk2yfs/58hwcAALCHoZ3TMzvDc1mS6zK4WtvJOyOm937jsPYLAAAw1zAvZHBRkkfM3vY+N8cUDgAAsCCGdsnq3vvbeu9tX7dh7RMAAGBvC/05PQAAAAtK9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlDbU6Gmt/XVr7brW2ndba1OttXe01k4b5j4BAADmGvZMz8eT/ESSRyV5QZKHJ/k/Q94nAADALkuGufHe++/PefqN1tpvJ/lAa+243vudw9w3AABAMuTomau1dlKSn0py+f6Cp7W2NMnSOYuWL8TYAACAuoZ+IYPW2u+01m5PcnOSM5P86L28/JIk03Num4c9PgAAoLZ5R09rbX1rrR/gdt6cVX43yROTXJRkR5K/aK21/Wz+dUnG59xOn+/4AAAA5mq99/mt0NrKJCsP8LKv996/u491T09yfZILeu+fOYh9jSWZnp6eztjY2LzGCQAA1DEzM5Px8fEkGe+9z8xn3Xmf09N735Jky3zXm7Vzhmfpvb4KAADgMBnahQxaa09K8qQkn0pya5KHJXlVkmuSHHCWBwAA4HAY5oUMtiV5fpJ/SPKVJH+W5EtJfqD3vn2I+wUAANhlaDM9vfcvJnn6sLYPAABwMIZ+yWoAAIBREj0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChN9AAAAKWJHgAAoDTRAwAAlCZ6AACA0kQPAABQmugBAABKEz0AAEBpogcAAChtQaKntba0tXZVa6231s5ZiH0CAAAkCzfT8/okNyzQvgAAAHYZevS01p6d5KIkvzrsfQEAAOxtyTA33lo7JcmmJD+WZOtBvH5pkqVzFi1PkpmZmWEMDwAAOErclyYYWvS01lqStyX5097751prZx3EapckWbf3wjPOOOPwDg4AADhanZRkXgXUeu/z2kNrbX32ESZ7OT/JBUlelOTC3vuO2ei5NskTe+9X7Wfb+5rp2Zzk9CS3zWugHC7eg9HzHoye92D0vAej5z0YLb/+o+c9GL2d78F4731e0XMoMz1vSvKeA7zm60lemeTJSbYPJn12+Vxr7V2995fsvVLvfXuS7Tufz1nvtvl+Yxwe3oPR8x6Mnvdg9LwHo+c9GC2//qPnPRi9vZpiXuYdPb33LUm2HOh1rbVfyiB8djotyd9lMPvz2fnuFwAA4FAM7Zye3vt1c5+31r4z+/Ca3vvmYe0XAABgroX6nJ5DtT3Jhsw55I0F5z0YPe/B6HkPRs97MHreg9Hy6z963oPRO+T3YN4XMgAAADiaHOkzPQAAAPeJ6AEAAEoTPQAAQGmiBwAAKO2oi57W2tLW2lWttd5aO2fU41lMWmt/3Vq7rrX23dbaVGvtHa2100Y9rsWitXZWa+2trbVrW2vbWmvXtNY2tNaOH/XYFpPW2itaa5e31ra21r496vEsBq21X5j9ff/d1tqVrbWnjnpMi0lr7cLW2gdbazfM/tv7Y6Me02LSWruktfbPrbXbWmvfaq19oLX2qFGPazFprb2stfaF1trM7O0zrbVnj3pci9Xsn4neWvuD+ax31EVPktcnuWHUg1ikPp7kJ5I8KskLkjw8yf8Z6YgWl0dn8Gd2bZLHJvl/k/y3JK8d5aAWoeOTvDfJn4x6IItBa+1FSf4gyW8leWKSf0zy4dbamaMc1yJzQpLPJ/nFUQ9kkfqBJG9O8uQkz8jgMxY/0lo7YaSjWlw2J/n1JOfN3j6W5NLW2mNHOqpFqLV2fpKXJvnCvNc9mi5ZPVvVb8jgB+5/TfLE3vtVIx3UItZaW53kA0mW9t7vHPFwFqXW2q8leVnv/WGjHsti01q7OMkf9N4fMOKhlNZa+2ySyd77y+YsuzrJB3rvl4xuZItTa60neV7v/QOjHsti1Vo7Ocm3kvxA7/2Tox7PYtVauyXJr/Xe3zrqsSwWrbUTk0wm+YUkr0xyVe/9lw92/aNmpqe1dkqSTUl+JsnWEQ9n0WutnZTkp5JcLnhGajzJLaMeBAzD7KGb5yb5yF5f+kiSCxZ+RHBEGJ+993f/CLTWjm2tvTiDGdDPjHo8i8ybk3yo9/7RQ1n5qIie1lpL8rYkf9p7/9yIh7OotdZ+p7V2e5Kbk5yZ5EdHPKRFq7X28CQvT/Knox4LDMnKJMcm+eZey7+Z5NSFHw6M1uzPQ29I8qne+5dGPZ7FpLX2uNbad5Jsz+Df3ef13v9txMNaNGZD89wkhzzDP9Loaa2tnz0R6d5u52Xwg91YkteNcrwVzeM92Ol3Mziu/qIkO5L8xexfwhyiQ3gPMnsBib9N8t7e+1tGM/I6DuU9YEHtfRx228cyWAzelOTxSX5y1ANZhL6S5JwMzq36kyRvb609ZqQjWiRaa2ck+cMkP9V7/+4hb2eU5/S01lZm8D959+brSd6T5Eey5z9yx2bwQ/e7eu8vGcoAF4GDfQ/29ZustXZ6kuuTXNB7N8V7iOb7HswGz8eTfDbJxb33u4c8xPIO5c+Bc3qGb/bwtq1Jfrz3/v45y/8wyTm99x8Y2eAWKef0jE5r7Y1JfizJhb33a0c8nEWvtfbRJNf03teOeizVzV4x8v0Z/Ny/07EZdMHdGZxbvmMfq+5hyVBGd5B671uSbDnQ61prv5TBCUs7nZbk75K8KIMf/DhEB/se7MfOGZ6lh2k4i9J83oPW2oMzCJ4rk/ys4Dk87uOfA4ak935Ha+3KDK5Y9f45X3pGkktHMypYWLNHU7wxyfOSPE3wHDFa/PyzUP4hyeP2WvbnSb6c5HcOJniSEUfPweq9Xzf3+ewxlcmgsDePYEiLTmvtSUmelORTSW5N8rAkr0pyTZzItyBmZ3guS3Jdkl9NcvLOIwt77zeObmSLy+ylkk/K4Jy2Y9vuzwv7Wu/9O/tdkUP1hiTvaK19LoO/a16awa+9c9kWyOwVkx4xZ9FDZ3/f37L3v88MxZuT/NcMzqG9rbW283y26d77ttENa/Forb02yYczOLpleZIXJ3lakmeNcFiLRu/9tiR7nMO28/zy+ZzbdlRED0eEbUmen2RDBlcsmcrgnJIX9963j3Jgi8hFGfzg8YgMPjNgLudVLZxXJZl7SO2/zN7/YAZRymHUe//frbUVSX4zyUQG//A9p/f+jdGObFE5L4MZ5p3eMHv/9iQXL/hoFp+dl2u/bK/lP5vBRZ4YvlOSvCODv4OmM/iMmGf13v9+pKNiXo6qz+kBAACYr6PiktUAAACHSvQAAACliR4AAKA00QMAAJQmegAAgNJEDwAAUJroAQAAShM9AABAaaIHAAAoTfQAAACliR4AAKA00QMAAJT2fwFK1GwtRTmh1gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "#env = gym.make('CartPole-v1')\n",
    "env = WarthogEnv('sim_remote_waypoint.txt', 'entropy_manual.csv')\n",
    "sizes = [128]\n",
    "obs_dimension = env.observation_space.shape\n",
    "action_dimension = env.action_space.shape\n",
    "pi = PolicyNetworkGauss(*obs_dimension, sizes, *action_dimension)\n",
    "o = torch.zeros(1,42)\n",
    "a = torch.as_tensor([0.5, 0.3], dtype=torch.float32)\n",
    "print(o)\n",
    "m = pi(o)\n",
    "logp = m.log_prob(a)\n",
    "print(len(a.shape))\n",
    "print(m)\n",
    "print(logp)\n",
    "print(m.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        #self.act_buf = np.zeros((size,), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size,act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "       # print(rews)\n",
    "       # print(vals)\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "\n",
    "        self.adv_buf = (self.adv_buf - self.adv_buf.mean()) / (self.adv_buf.std()+eps)\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env, seed = 0, buff_size = 128, batch_size = 4, train_time_steps = 10000000, gamma = 0.99, clip_ratio = 0.2, lr_pi = 3e-5, \n",
    "        lr_vf = 1e-3, pi_train_itrs = 4, v_train_itrs = 4, lam = 0.97, max_ep_len = 500, ent_coeff = 0.01):\n",
    "        obs_dim = env.observation_space.shape\n",
    "        action_dim = env.action_space.shape\n",
    "        h_sizes = [64,64]\n",
    "        vi = ValueNetwork(*obs_dim, h_sizes, act=nn.Tanh).to(device)\n",
    "        pi = PolicyNetworkGauss(*obs_dim, h_sizes, *action_dim,act=nn.Tanh).to(device)\n",
    "        data_buff = PPOBuffer(*obs_dim, *action_dim, buff_size)\n",
    "        policy_opt = optim.Adam(pi.parameters(), lr = lr_pi, eps=1e-5)\n",
    "        value_opt = optim.Adam(vi.parameters(), lr = lr_vf, eps=1e-5)\n",
    "        obs = env.reset()\n",
    "        curr_time_step = 0\n",
    "        num_episode = 0\n",
    "        ep_rewards = [0]\n",
    "        ep_steps = [0]\n",
    "        start_time = time.time()\n",
    "        while curr_time_step < train_time_steps: \n",
    "                for t in range(0, buff_size):\n",
    "                        curr_time_step+=1\n",
    "                        with torch.no_grad():\n",
    "                                m = pi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                action = m.sample()\n",
    "                                action = action.reshape((-1,) + action_dim)\n",
    "                                logp = m.log_prob(action).sum(dim=1)\n",
    "                                action = action.cpu().numpy() \n",
    "                                clipped_action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "                                obs_new, rew, done, _ = env.step(clipped_action[0])\n",
    "                                ep_rewards[num_episode]+=rew \n",
    "                                ep_steps[num_episode]+=1\n",
    "                                v = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                        data_buff.store(obs, action, rew, v.cpu().numpy(), logp.cpu().numpy())\n",
    "                        obs = obs_new\n",
    "                        if done or buff_size-1:\n",
    "                                if done:\n",
    "                                        v_ = 0.\n",
    "                                        obs = env.reset()\n",
    "                                        done = False\n",
    "                                        num_episode+=1\n",
    "                                        ep_rewards.append(0)\n",
    "                                        ep_steps.append(0)\n",
    "                                        curr_time = time.time()\n",
    "                                        if num_episode %100 == 0:\n",
    "                                                print(f'episode: {num_episode-1} \\t episode_reward: {np.mean(ep_rewards[-10:-2])} \\t total steps:{curr_time_step}\\t fps\"{curr_time_step/(curr_time-start_time)}\\t avg_ep_steps: {np.mean(ep_steps[-10:-2])}')\n",
    "                                                torch.save(pi, f'./temp_policy/tan_h/manaul_entropy3_ppo_batch_{curr_time_step}_rew_{int(np.mean(ep_rewards[-10:-2]))}.pt')\n",
    "                                else:\n",
    "                                        v_ = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                        v_ = v_.detach().cpu().numpy()\n",
    "                                data_buff.finish_path(v_)\n",
    "                        if(curr_time_step % 100000 == 0):\n",
    "                                np.savetxt(f'./temp_policy/tan_h/avg_rew_{curr_time_step}', ep_rewards, fmt='%f')\n",
    "\n",
    "                data = data_buff.get()\n",
    "                ret, act, adv, o, logp_old= data['ret'].to(device), data['act'].to(device), data['adv'].to(device), data['obs'].to(device), data['logp'].to(device)\n",
    "                for j in range(0, pi_train_itrs):\n",
    "                        indices = np.random.permutation(buff_size)\n",
    "                        start_idx = 0\n",
    "                        while start_idx < buff_size:\n",
    "\n",
    "                                batch_ind = indices[start_idx: start_idx+batch_size]\n",
    "                                ret_b = ret[batch_ind]\n",
    "                                act_b = act[batch_ind]\n",
    "                                adv_b = adv[batch_ind]\n",
    "                                o_b = o[batch_ind]\n",
    "                                logp_old_b = logp_old[batch_ind]\n",
    "                                act_dist = pi(o_b)\n",
    "                                logp = act_dist.log_prob(act_b).sum(dim=1)\n",
    "                                ratio = torch.exp(logp - logp_old_b)\n",
    "                                clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv_b\n",
    "                                loss_pi = -(torch.min(ratio * adv_b, clip_adv)).mean() - ent_coeff* torch.mean(-logp)\n",
    "                                policy_opt.zero_grad()\n",
    "                                loss_pi.backward()\n",
    "                                policy_opt.step()\n",
    "                                val = vi(o_b)\n",
    "                                value_loss = F.mse_loss(val.flatten(), ret_b)\n",
    "                                value_opt.zero_grad()\n",
    "                                value_loss.backward()\n",
    "                                value_opt.step()\n",
    "                                start_idx = start_idx+batch_size\n",
    "        return pi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 99 \t episode_reward: 139.74565687397524 \t total steps:18200\t fps\"521.5645945874409\t avg_ep_steps: 155.375\n",
      "episode: 199 \t episode_reward: 319.8121485488325 \t total steps:49000\t fps\"516.1574030006099\t avg_ep_steps: 487.25\n",
      "episode: 299 \t episode_reward: 328.65165020413576 \t total steps:77954\t fps\"514.5243260225202\t avg_ep_steps: 300.5\n",
      "episode: 399 \t episode_reward: 675.6309624841399 \t total steps:114715\t fps\"516.4214437362359\t avg_ep_steps: 373.125\n",
      "episode: 499 \t episode_reward: 593.0379209413359 \t total steps:148004\t fps\"517.335973222792\t avg_ep_steps: 261.875\n",
      "episode: 599 \t episode_reward: 1292.0441199611605 \t total steps:184041\t fps\"518.1460171577543\t avg_ep_steps: 405.625\n",
      "episode: 699 \t episode_reward: 1318.3972419149877 \t total steps:224500\t fps\"518.768631908248\t avg_ep_steps: 351.625\n",
      "episode: 799 \t episode_reward: 778.8913036300914 \t total steps:268049\t fps\"517.9414232169688\t avg_ep_steps: 242.375\n",
      "episode: 899 \t episode_reward: 680.7495270348198 \t total steps:296100\t fps\"518.0277209367943\t avg_ep_steps: 169.5\n",
      "episode: 999 \t episode_reward: 2830.7733648427 \t total steps:347628\t fps\"518.1408779293228\t avg_ep_steps: 619.125\n",
      "episode: 1099 \t episode_reward: 2261.684933718371 \t total steps:385000\t fps\"518.0800730604448\t avg_ep_steps: 504.375\n",
      "episode: 1199 \t episode_reward: 687.7090539364833 \t total steps:431514\t fps\"518.887583030702\t avg_ep_steps: 216.625\n",
      "episode: 1299 \t episode_reward: 992.6561976490063 \t total steps:473744\t fps\"520.7129481800564\t avg_ep_steps: 234.0\n",
      "episode: 1399 \t episode_reward: 1424.1312144149192 \t total steps:520100\t fps\"518.6123941060589\t avg_ep_steps: 350.0\n",
      "episode: 1499 \t episode_reward: 3181.221890139412 \t total steps:564900\t fps\"512.3948702644988\t avg_ep_steps: 636.125\n",
      "episode: 1599 \t episode_reward: 1391.6806416485115 \t total steps:600708\t fps\"508.65774778436713\t avg_ep_steps: 377.0\n",
      "episode: 1699 \t episode_reward: 1087.7152939254 \t total steps:632892\t fps\"505.6379616617348\t avg_ep_steps: 262.5\n",
      "episode: 1799 \t episode_reward: 2507.0280485342737 \t total steps:676900\t fps\"501.7680004670158\t avg_ep_steps: 612.5\n",
      "episode: 1899 \t episode_reward: 722.1327561916769 \t total steps:718622\t fps\"498.58176340620014\t avg_ep_steps: 269.5\n",
      "episode: 1999 \t episode_reward: 608.0111892481514 \t total steps:745835\t fps\"496.9905878890933\t avg_ep_steps: 199.375\n",
      "episode: 2099 \t episode_reward: 373.9614156199698 \t total steps:765970\t fps\"496.05731540949915\t avg_ep_steps: 110.375\n",
      "episode: 2199 \t episode_reward: 95.82513258370817 \t total steps:774092\t fps\"496.20824505670726\t avg_ep_steps: 50.625\n",
      "episode: 2299 \t episode_reward: 248.04494110710505 \t total steps:780747\t fps\"496.4294628206526\t avg_ep_steps: 108.875\n",
      "episode: 2399 \t episode_reward: 245.07409049057856 \t total steps:787500\t fps\"496.5722628333333\t avg_ep_steps: 86.625\n",
      "episode: 2499 \t episode_reward: 561.8364475561855 \t total steps:797679\t fps\"496.5967562823379\t avg_ep_steps: 175.875\n",
      "episode: 2599 \t episode_reward: 255.77997234167034 \t total steps:807800\t fps\"496.51554548559795\t avg_ep_steps: 96.75\n",
      "episode: 2699 \t episode_reward: 1121.7088334487435 \t total steps:821667\t fps\"496.0809993485594\t avg_ep_steps: 314.5\n",
      "episode: 2799 \t episode_reward: 794.5952002486858 \t total steps:856443\t fps\"494.21604688186625\t avg_ep_steps: 197.875\n",
      "episode: 2899 \t episode_reward: 1589.6332066787922 \t total steps:893900\t fps\"492.3799945790571\t avg_ep_steps: 378.375\n",
      "episode: 2999 \t episode_reward: 1793.611101568046 \t total steps:930415\t fps\"490.6791655297178\t avg_ep_steps: 437.5\n",
      "episode: 3099 \t episode_reward: 1111.0921339799602 \t total steps:961891\t fps\"489.49629177352267\t avg_ep_steps: 315.375\n",
      "episode: 3199 \t episode_reward: 1623.8213492066047 \t total steps:995400\t fps\"488.242157996587\t avg_ep_steps: 508.625\n",
      "episode: 3299 \t episode_reward: 1366.563990285621 \t total steps:1029994\t fps\"487.12382643557874\t avg_ep_steps: 281.125\n",
      "episode: 3399 \t episode_reward: 2083.7896558063976 \t total steps:1070959\t fps\"485.7800499071754\t avg_ep_steps: 437.5\n",
      "episode: 3499 \t episode_reward: 785.5937732067783 \t total steps:1087952\t fps\"485.4305567799335\t avg_ep_steps: 183.875\n",
      "episode: 3599 \t episode_reward: 1760.064736159421 \t total steps:1134000\t fps\"484.0423393389051\t avg_ep_steps: 435.375\n",
      "episode: 3699 \t episode_reward: 1320.5150874489782 \t total steps:1168038\t fps\"483.1124090922905\t avg_ep_steps: 318.625\n",
      "episode: 3799 \t episode_reward: 1618.3198710294218 \t total steps:1197527\t fps\"482.50912649916364\t avg_ep_steps: 483.0\n",
      "episode: 3899 \t episode_reward: 306.93175303999806 \t total steps:1207280\t fps\"482.5638925484749\t avg_ep_steps: 90.0\n",
      "episode: 3999 \t episode_reward: 902.0857396484093 \t total steps:1221723\t fps\"482.4331469847792\t avg_ep_steps: 181.875\n",
      "episode: 4099 \t episode_reward: 389.9364927997668 \t total steps:1238515\t fps\"482.18794944377066\t avg_ep_steps: 103.25\n",
      "episode: 4199 \t episode_reward: 1321.491106846295 \t total steps:1265822\t fps\"481.5306629098456\t avg_ep_steps: 283.25\n",
      "episode: 4299 \t episode_reward: 394.86551622714944 \t total steps:1285556\t fps\"481.26363141659994\t avg_ep_steps: 93.5\n",
      "episode: 4399 \t episode_reward: 256.32116033167836 \t total steps:1293951\t fps\"481.42812082666046\t avg_ep_steps: 78.25\n",
      "episode: 4499 \t episode_reward: 774.4383934437947 \t total steps:1307338\t fps\"481.34341237634436\t avg_ep_steps: 154.875\n",
      "episode: 4599 \t episode_reward: 1488.2313583637206 \t total steps:1330046\t fps\"480.91511681059336\t avg_ep_steps: 315.625\n",
      "episode: 4699 \t episode_reward: 985.7129371887922 \t total steps:1358662\t fps\"480.301251907306\t avg_ep_steps: 228.875\n",
      "episode: 4799 \t episode_reward: 1330.6447764630939 \t total steps:1388856\t fps\"479.7058207715676\t avg_ep_steps: 328.75\n",
      "episode: 4899 \t episode_reward: 1262.9600526525471 \t total steps:1409976\t fps\"479.39172047621304\t avg_ep_steps: 297.875\n",
      "episode: 4999 \t episode_reward: 313.1886149447265 \t total steps:1433548\t fps\"479.0252583805314\t avg_ep_steps: 107.75\n",
      "episode: 5099 \t episode_reward: 209.00123927967445 \t total steps:1443000\t fps\"479.1522704052676\t avg_ep_steps: 61.875\n",
      "episode: 5199 \t episode_reward: 310.583038915157 \t total steps:1450357\t fps\"479.3370413151843\t avg_ep_steps: 97.875\n",
      "episode: 5299 \t episode_reward: 313.6420564674344 \t total steps:1462440\t fps\"479.28103372679976\t avg_ep_steps: 83.5\n",
      "episode: 5399 \t episode_reward: 1361.3042669524887 \t total steps:1486100\t fps\"478.9407816535445\t avg_ep_steps: 314.875\n",
      "episode: 5499 \t episode_reward: 352.784089301742 \t total steps:1503565\t fps\"478.7880109041739\t avg_ep_steps: 95.0\n",
      "episode: 5599 \t episode_reward: 213.48183548375957 \t total steps:1509772\t fps\"478.8991327598177\t avg_ep_steps: 62.125\n",
      "episode: 5699 \t episode_reward: 166.2184516480848 \t total steps:1516200\t fps\"479.0300147837642\t avg_ep_steps: 54.75\n",
      "episode: 5799 \t episode_reward: 265.56318867958845 \t total steps:1522791\t fps\"479.1682935084603\t avg_ep_steps: 66.75\n",
      "episode: 5899 \t episode_reward: 230.91683344478056 \t total steps:1530728\t fps\"479.2519393943578\t avg_ep_steps: 71.375\n",
      "episode: 5999 \t episode_reward: 718.4553369167638 \t total steps:1557862\t fps\"478.83447342989166\t avg_ep_steps: 207.0\n",
      "episode: 6099 \t episode_reward: 1796.3209194331998 \t total steps:1580324\t fps\"478.58034229411544\t avg_ep_steps: 465.375\n",
      "episode: 6199 \t episode_reward: 512.4786086804452 \t total steps:1595495\t fps\"478.56499260496383\t avg_ep_steps: 144.125\n",
      "episode: 6299 \t episode_reward: 165.20201065835542 \t total steps:1606156\t fps\"478.64239531295385\t avg_ep_steps: 60.25\n",
      "episode: 6399 \t episode_reward: 76.29337412366843 \t total steps:1613041\t fps\"478.7820137488967\t avg_ep_steps: 52.0\n",
      "episode: 6499 \t episode_reward: 229.82471783437416 \t total steps:1620529\t fps\"478.95252916113424\t avg_ep_steps: 77.25\n",
      "episode: 6599 \t episode_reward: 250.975977407985 \t total steps:1630300\t fps\"479.09759306978077\t avg_ep_steps: 98.75\n",
      "episode: 6699 \t episode_reward: 248.39302071541124 \t total steps:1637837\t fps\"479.2713624966831\t avg_ep_steps: 80.0\n",
      "episode: 6799 \t episode_reward: 138.51728171887083 \t total steps:1644729\t fps\"479.4177747579988\t avg_ep_steps: 53.75\n",
      "episode: 6899 \t episode_reward: 195.3259016511755 \t total steps:1650494\t fps\"479.5947260233168\t avg_ep_steps: 58.5\n",
      "episode: 6999 \t episode_reward: 97.62652845705242 \t total steps:1655994\t fps\"479.76544874268865\t avg_ep_steps: 58.625\n",
      "episode: 7099 \t episode_reward: 101.20373352815903 \t total steps:1661533\t fps\"479.9393484599506\t avg_ep_steps: 48.25\n",
      "episode: 7199 \t episode_reward: 224.49191678026406 \t total steps:1666938\t fps\"480.10352078892777\t avg_ep_steps: 61.75\n",
      "episode: 7299 \t episode_reward: 164.1802586865553 \t total steps:1672684\t fps\"480.26858940614585\t avg_ep_steps: 65.75\n",
      "episode: 7399 \t episode_reward: 174.7361091567289 \t total steps:1678275\t fps\"480.4163515138662\t avg_ep_steps: 63.25\n",
      "episode: 7499 \t episode_reward: 127.22897198091161 \t total steps:1684486\t fps\"480.5872347775873\t avg_ep_steps: 44.625\n",
      "episode: 7599 \t episode_reward: 169.2067747173597 \t total steps:1690417\t fps\"480.7422191951169\t avg_ep_steps: 68.5\n",
      "episode: 7699 \t episode_reward: 185.25265136259475 \t total steps:1696446\t fps\"480.9091268166061\t avg_ep_steps: 64.875\n",
      "episode: 7799 \t episode_reward: 149.5472726316972 \t total steps:1701870\t fps\"481.0554417742389\t avg_ep_steps: 59.375\n",
      "episode: 7899 \t episode_reward: 139.88164018228724 \t total steps:1706991\t fps\"481.1981175187457\t avg_ep_steps: 49.875\n",
      "episode: 7999 \t episode_reward: 157.23822000366724 \t total steps:1713011\t fps\"481.334000731479\t avg_ep_steps: 54.125\n",
      "episode: 8099 \t episode_reward: 178.67785464123128 \t total steps:1719103\t fps\"481.42722649784594\t avg_ep_steps: 59.75\n",
      "episode: 8199 \t episode_reward: 418.40472704454464 \t total steps:1725573\t fps\"481.52934941447216\t avg_ep_steps: 91.375\n",
      "episode: 8299 \t episode_reward: 210.41457816184572 \t total steps:1731033\t fps\"481.6850621516011\t avg_ep_steps: 63.25\n",
      "episode: 8399 \t episode_reward: 182.00424228249494 \t total steps:1736834\t fps\"481.79997104014905\t avg_ep_steps: 58.75\n",
      "episode: 8499 \t episode_reward: 214.85178871305044 \t total steps:1743823\t fps\"481.9434860384644\t avg_ep_steps: 53.75\n",
      "episode: 8599 \t episode_reward: 371.70878158903633 \t total steps:1757896\t fps\"481.92668553897255\t avg_ep_steps: 96.5\n",
      "episode: 8699 \t episode_reward: 757.1941693363905 \t total steps:1772551\t fps\"481.8655165987426\t avg_ep_steps: 157.625\n",
      "episode: 8799 \t episode_reward: 1491.9817448449276 \t total steps:1799325\t fps\"481.46095373513543\t avg_ep_steps: 344.5\n",
      "episode: 8899 \t episode_reward: 1170.6004158287978 \t total steps:1827482\t fps\"480.9748908159714\t avg_ep_steps: 233.375\n",
      "episode: 8999 \t episode_reward: 1602.7422923491486 \t total steps:1852387\t fps\"480.67503160477906\t avg_ep_steps: 351.0\n",
      "episode: 9099 \t episode_reward: 683.4681263391792 \t total steps:1889708\t fps\"480.0748722553905\t avg_ep_steps: 266.375\n",
      "episode: 9199 \t episode_reward: 188.83004833899344 \t total steps:1898021\t fps\"480.157624522434\t avg_ep_steps: 54.0\n",
      "episode: 9299 \t episode_reward: 198.02168021271137 \t total steps:1904514\t fps\"480.2268005021706\t avg_ep_steps: 67.25\n",
      "episode: 9399 \t episode_reward: 135.48801747330288 \t total steps:1910363\t fps\"480.3563137461414\t avg_ep_steps: 52.625\n",
      "episode: 9499 \t episode_reward: 259.4092843578351 \t total steps:1916847\t fps\"480.42741352282485\t avg_ep_steps: 72.375\n",
      "episode: 9599 \t episode_reward: 384.2154004307936 \t total steps:1923722\t fps\"480.4902481226316\t avg_ep_steps: 88.25\n",
      "episode: 9699 \t episode_reward: 214.82813706681188 \t total steps:1931592\t fps\"480.60149655699666\t avg_ep_steps: 63.5\n",
      "episode: 9799 \t episode_reward: 256.2722083209912 \t total steps:1938838\t fps\"480.7162604337458\t avg_ep_steps: 71.5\n",
      "episode: 9899 \t episode_reward: 163.49111799535478 \t total steps:1945483\t fps\"480.85689397216777\t avg_ep_steps: 73.5\n",
      "episode: 9999 \t episode_reward: 146.16873026211982 \t total steps:1951249\t fps\"480.9788758250886\t avg_ep_steps: 53.5\n",
      "episode: 10099 \t episode_reward: 473.88190898257636 \t total steps:1957585\t fps\"481.08975172173155\t avg_ep_steps: 127.25\n",
      "episode: 10199 \t episode_reward: 215.70783079415125 \t total steps:1964320\t fps\"481.21299182831615\t avg_ep_steps: 54.75\n",
      "episode: 10299 \t episode_reward: 214.23607159907064 \t total steps:1971200\t fps\"481.3515070187681\t avg_ep_steps: 70.625\n",
      "episode: 10399 \t episode_reward: 261.7992005959898 \t total steps:1978106\t fps\"481.44672981327176\t avg_ep_steps: 78.75\n",
      "episode: 10499 \t episode_reward: 282.9052151629065 \t total steps:1985200\t fps\"481.59266386809674\t avg_ep_steps: 73.875\n",
      "episode: 10599 \t episode_reward: 416.4815212156017 \t total steps:1997754\t fps\"481.5993624609099\t avg_ep_steps: 89.5\n",
      "episode: 10699 \t episode_reward: 637.0262938992998 \t total steps:2009700\t fps\"481.6163771527748\t avg_ep_steps: 144.625\n",
      "episode: 10799 \t episode_reward: 479.45771636794296 \t total steps:2018920\t fps\"481.7382397165948\t avg_ep_steps: 129.375\n",
      "episode: 10899 \t episode_reward: 192.28730113856454 \t total steps:2026500\t fps\"481.8410066034265\t avg_ep_steps: 77.25\n",
      "episode: 10999 \t episode_reward: 166.05062385651388 \t total steps:2032691\t fps\"481.9686179931231\t avg_ep_steps: 57.25\n",
      "episode: 11099 \t episode_reward: 92.50960470029392 \t total steps:2038694\t fps\"482.07434370381657\t avg_ep_steps: 34.125\n",
      "episode: 11199 \t episode_reward: 156.3264769166147 \t total steps:2043980\t fps\"482.1967269703803\t avg_ep_steps: 70.125\n",
      "episode: 11299 \t episode_reward: 204.1776967659781 \t total steps:2049771\t fps\"482.322979086352\t avg_ep_steps: 64.5\n",
      "episode: 11399 \t episode_reward: 189.3124544449533 \t total steps:2054996\t fps\"482.4203443246835\t avg_ep_steps: 57.25\n",
      "episode: 11499 \t episode_reward: 219.28697420339842 \t total steps:2060453\t fps\"482.53358515761863\t avg_ep_steps: 67.375\n",
      "episode: 11599 \t episode_reward: 165.60889369965466 \t total steps:2066255\t fps\"482.64296948643147\t avg_ep_steps: 56.0\n",
      "episode: 11699 \t episode_reward: 316.24519409324296 \t total steps:2072277\t fps\"482.71102445910265\t avg_ep_steps: 74.125\n",
      "episode: 11799 \t episode_reward: 207.5584068910409 \t total steps:2077916\t fps\"482.8230993284332\t avg_ep_steps: 54.125\n",
      "episode: 11899 \t episode_reward: 216.28798679954076 \t total steps:2083641\t fps\"482.9404358742989\t avg_ep_steps: 65.25\n",
      "episode: 11999 \t episode_reward: 108.3957233231036 \t total steps:2089423\t fps\"483.0490137682486\t avg_ep_steps: 55.875\n",
      "episode: 12099 \t episode_reward: 174.6230125138934 \t total steps:2094530\t fps\"483.131742395259\t avg_ep_steps: 53.5\n",
      "episode: 12199 \t episode_reward: 191.57045795349205 \t total steps:2100594\t fps\"483.24254596455467\t avg_ep_steps: 67.125\n",
      "episode: 12299 \t episode_reward: 87.5918603099025 \t total steps:2107067\t fps\"483.32517430756246\t avg_ep_steps: 35.75\n",
      "episode: 12399 \t episode_reward: 177.01715776536864 \t total steps:2113074\t fps\"483.41262531382876\t avg_ep_steps: 62.25\n",
      "episode: 12499 \t episode_reward: 271.7536751834446 \t total steps:2121000\t fps\"483.5059407244996\t avg_ep_steps: 75.5\n",
      "episode: 12599 \t episode_reward: 115.2553660392546 \t total steps:2128490\t fps\"483.63458860381417\t avg_ep_steps: 54.75\n",
      "episode: 12699 \t episode_reward: 133.7467119514223 \t total steps:2134664\t fps\"483.7545154137563\t avg_ep_steps: 60.25\n",
      "episode: 12799 \t episode_reward: 115.42340468850807 \t total steps:2140945\t fps\"483.85497495024975\t avg_ep_steps: 44.25\n",
      "episode: 12899 \t episode_reward: 176.61653659533926 \t total steps:2147497\t fps\"483.9934766556625\t avg_ep_steps: 61.25\n",
      "episode: 12999 \t episode_reward: 150.48398349996813 \t total steps:2153239\t fps\"484.11611671487833\t avg_ep_steps: 63.5\n",
      "episode: 13099 \t episode_reward: 138.0176937012771 \t total steps:2159128\t fps\"484.22971941689184\t avg_ep_steps: 54.0\n",
      "episode: 13199 \t episode_reward: 152.835221137484 \t total steps:2165756\t fps\"484.3408479856194\t avg_ep_steps: 55.75\n",
      "episode: 13299 \t episode_reward: 139.07577682474218 \t total steps:2172616\t fps\"484.4332881841014\t avg_ep_steps: 58.375\n",
      "episode: 13399 \t episode_reward: 181.17279390637202 \t total steps:2179595\t fps\"484.5395072531431\t avg_ep_steps: 88.5\n",
      "episode: 13499 \t episode_reward: 261.77471403480774 \t total steps:2187570\t fps\"484.6705924217922\t avg_ep_steps: 87.5\n",
      "episode: 13599 \t episode_reward: 202.05621079144052 \t total steps:2196538\t fps\"484.83222010010087\t avg_ep_steps: 72.375\n",
      "episode: 13699 \t episode_reward: 163.45929527537996 \t total steps:2202900\t fps\"484.9241018140549\t avg_ep_steps: 71.75\n",
      "episode: 13799 \t episode_reward: 249.9310728032916 \t total steps:2210052\t fps\"485.01296740515875\t avg_ep_steps: 86.0\n",
      "episode: 13899 \t episode_reward: 401.8300229288784 \t total steps:2217711\t fps\"485.13381409201776\t avg_ep_steps: 111.25\n",
      "episode: 13999 \t episode_reward: 282.2588283570965 \t total steps:2226861\t fps\"485.15133504351155\t avg_ep_steps: 95.25\n",
      "episode: 14099 \t episode_reward: 288.99984215565684 \t total steps:2235675\t fps\"485.2039228343335\t avg_ep_steps: 83.0\n",
      "episode: 14199 \t episode_reward: 306.91916175165136 \t total steps:2245600\t fps\"485.2286579643564\t avg_ep_steps: 77.625\n",
      "episode: 14299 \t episode_reward: 283.9391830342066 \t total steps:2253946\t fps\"485.3054638269195\t avg_ep_steps: 80.375\n",
      "episode: 14399 \t episode_reward: 239.15008146672056 \t total steps:2262092\t fps\"485.3604974495613\t avg_ep_steps: 64.125\n",
      "episode: 14499 \t episode_reward: 391.6612017064749 \t total steps:2269400\t fps\"485.4510771401452\t avg_ep_steps: 92.75\n",
      "episode: 14599 \t episode_reward: 175.4257039450979 \t total steps:2276694\t fps\"485.5101946654564\t avg_ep_steps: 58.0\n",
      "episode: 14699 \t episode_reward: 215.7365534048469 \t total steps:2283800\t fps\"485.56700381115746\t avg_ep_steps: 54.375\n",
      "episode: 14799 \t episode_reward: 293.7090190474669 \t total steps:2291100\t fps\"485.65382723748075\t avg_ep_steps: 74.25\n",
      "episode: 14899 \t episode_reward: 202.32259316410088 \t total steps:2298733\t fps\"485.6797762054515\t avg_ep_steps: 66.875\n",
      "episode: 14999 \t episode_reward: 541.520037930374 \t total steps:2308278\t fps\"485.6763326745486\t avg_ep_steps: 112.75\n",
      "episode: 15099 \t episode_reward: 185.51538343277764 \t total steps:2320253\t fps\"485.60515634708634\t avg_ep_steps: 56.625\n",
      "episode: 15199 \t episode_reward: 224.41751753669664 \t total steps:2328665\t fps\"485.5939007343048\t avg_ep_steps: 81.0\n",
      "episode: 15299 \t episode_reward: 1392.6401363956288 \t total steps:2347984\t fps\"485.3705570554004\t avg_ep_steps: 240.875\n",
      "episode: 15399 \t episode_reward: 2065.719670633162 \t total steps:2373855\t fps\"485.0444022812176\t avg_ep_steps: 480.125\n",
      "episode: 15499 \t episode_reward: 487.14513139085204 \t total steps:2395681\t fps\"484.79231014428854\t avg_ep_steps: 102.5\n",
      "episode: 15599 \t episode_reward: 696.8218156491978 \t total steps:2410380\t fps\"484.7102107200532\t avg_ep_steps: 127.875\n",
      "episode: 15699 \t episode_reward: 697.6168998327505 \t total steps:2423970\t fps\"484.64758348616516\t avg_ep_steps: 155.25\n",
      "episode: 15799 \t episode_reward: 438.0371470098462 \t total steps:2439292\t fps\"484.540210093105\t avg_ep_steps: 127.375\n",
      "episode: 15899 \t episode_reward: 440.3148079492979 \t total steps:2452003\t fps\"484.4596459224878\t avg_ep_steps: 95.0\n",
      "episode: 15999 \t episode_reward: 412.42142822868226 \t total steps:2462816\t fps\"484.4383672802444\t avg_ep_steps: 97.625\n",
      "episode: 16099 \t episode_reward: 223.0247321606791 \t total steps:2474070\t fps\"484.40864210855074\t avg_ep_steps: 66.875\n",
      "episode: 16199 \t episode_reward: 726.4430274261308 \t total steps:2486306\t fps\"484.3942547611245\t avg_ep_steps: 162.875\n",
      "episode: 16299 \t episode_reward: 282.11163226552173 \t total steps:2497851\t fps\"484.3651478752805\t avg_ep_steps: 94.875\n",
      "episode: 16399 \t episode_reward: 456.8084956387876 \t total steps:2508837\t fps\"484.33998061261565\t avg_ep_steps: 128.25\n",
      "episode: 16499 \t episode_reward: 637.2389847895344 \t total steps:2528892\t fps\"484.11713071502794\t avg_ep_steps: 174.0\n",
      "episode: 16599 \t episode_reward: 681.6579558852923 \t total steps:2542012\t fps\"484.04997965368347\t avg_ep_steps: 137.0\n",
      "episode: 16699 \t episode_reward: 387.23413139387253 \t total steps:2555169\t fps\"483.9654495416059\t avg_ep_steps: 102.125\n",
      "episode: 16799 \t episode_reward: 205.7056476467733 \t total steps:2566392\t fps\"483.96297035397396\t avg_ep_steps: 70.125\n",
      "episode: 16899 \t episode_reward: 317.6473237572112 \t total steps:2576983\t fps\"483.994084861512\t avg_ep_steps: 115.75\n",
      "episode: 16999 \t episode_reward: 400.87331735794817 \t total steps:2588988\t fps\"483.9737951933326\t avg_ep_steps: 93.875\n",
      "episode: 17099 \t episode_reward: 284.2078909367277 \t total steps:2602506\t fps\"483.81215282408454\t avg_ep_steps: 92.625\n",
      "episode: 17199 \t episode_reward: 692.5534912846065 \t total steps:2616161\t fps\"483.755793156709\t avg_ep_steps: 174.625\n",
      "episode: 17299 \t episode_reward: 738.5169059586983 \t total steps:2633550\t fps\"483.6242501846292\t avg_ep_steps: 175.0\n",
      "episode: 17399 \t episode_reward: 400.5974389257301 \t total steps:2651600\t fps\"483.4278997973249\t avg_ep_steps: 136.625\n",
      "episode: 17499 \t episode_reward: 190.66607525050802 \t total steps:2662950\t fps\"483.4033323313614\t avg_ep_steps: 87.375\n",
      "episode: 17599 \t episode_reward: 2506.171666221513 \t total steps:2688000\t fps\"483.13173693494116\t avg_ep_steps: 516.5\n",
      "episode: 17699 \t episode_reward: 1619.9871944328934 \t total steps:2717400\t fps\"482.78401319601306\t avg_ep_steps: 357.875\n",
      "episode: 17799 \t episode_reward: 1199.0548473802467 \t total steps:2745345\t fps\"482.49295889453776\t avg_ep_steps: 336.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\akhil\\AppData\\Local\\Temp\\ipykernel_35364\\4011267555.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#python.dataScience.textOutputLimit = 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\akhil\\AppData\\Local\\Temp\\ipykernel_35364\\864655220.py\u001b[0m in \u001b[0;36mppo\u001b[1;34m(env, seed, buff_size, batch_size, train_time_steps, gamma, clip_ratio, lr_pi, lr_vf, pi_train_itrs, v_train_itrs, lam, max_ep_len, ent_coeff)\u001b[0m\n\u001b[0;32m     19\u001b[0m                         \u001b[0mcurr_time_step\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                                 \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\akhil\\AppData\\Local\\Temp\\ipykernel_35364\\2421673568.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\rl\\lib\\site-packages\\torch\\distributions\\normal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\rl\\lib\\site-packages\\torch\\distributions\\distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy_property\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                     \u001b[1;32mcontinue\u001b[0m  \u001b[1;31m# skip checking lazily-constructed args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The parameter {} has invalid values\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\rl\\lib\\site-packages\\torch\\distributions\\constraints.py\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \"\"\"\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalue\u001b[0m  \u001b[1;31m# False for NANs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#python.dataScience.textOutputLimit = 0\n",
    "pi = ppo(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_label(train):\n",
    "    train_obs = train[:,0:42]\n",
    "    train_labels = train[:,42:44]\n",
    "    train_labels[:, 0] = train_labels[:,0]/4\n",
    "    train_labels[:, 1] = train_labels[:,1]/2.5\n",
    "    return train_obs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76401783 0.44555561 0.37034875 0.323793   0.29304121 0.27176312\n",
      " 0.25639162 0.24480111 0.23569725 0.22828266]\n"
     ]
    }
   ],
   "source": [
    "obs_file_name = \"./entropy_manual.csv\"\n",
    "df = pd.read_csv(obs_file_name, header=None)\n",
    "model_file_name = \"./temp_policy/tan_h/manaul_entropy2_ppo_batch_6916920_rew_3529.pt\"\n",
    "model = torch.load(model_file_name)\n",
    "obs_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.shape\n",
    "h_sizes = [64,64]\n",
    "#model = PolicyNetworkGauss(*obs_dim, h_sizes, *action_dim,act=nn.Tanh).to(device)\n",
    "num_data_points = len(df.index)\n",
    "df = df.to_numpy()\n",
    "num_obs = 10000\n",
    "df = df[0:num_obs]\n",
    "msk = np.random.rand(len(df)) < 0.98\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "#batch_size = 512 best batch size\n",
    "batch_size = 512\n",
    "num_batches = int(len(train)/batch_size)\n",
    "#num_batches = 1\n",
    "n_epochs = 10\n",
    "train_obs, train_label = get_obs_label(train)\n",
    "test_obs, test_label = get_obs_label(test)\n",
    "#opti = optim.Adam(model.parameters(), lr=0.0001)\n",
    "opti = optim.SGD(model.parameters(), lr=0.1)\n",
    "losses = np.zeros(n_epochs)\n",
    "for i in (range(n_epochs)):\n",
    "    for k in range(num_batches):\n",
    "        start_idx = k*batch_size\n",
    "        end_idx = min((k+1)*batch_size, len(train))\n",
    "        batch_obs = train_obs[start_idx:end_idx]\n",
    "        batch_label = train_label[start_idx:end_idx]\n",
    "        batch_obs_tensor = torch.flatten(torch.as_tensor(batch_obs, dtype=torch.float32), start_dim=1)\n",
    "        batch_label_tensor = torch.flatten(torch.as_tensor(batch_label, dtype=torch.float32), start_dim=1)\n",
    "        batch_target_tensor = model(batch_obs_tensor).loc \n",
    "        loss = F.mse_loss(batch_target_tensor, batch_label_tensor)\n",
    "        losses[i] = losses[i] + loss.item()\n",
    "        #print(batch_target[0:3])\n",
    "        ##print(batch_label[0:3])\n",
    "      #  print(loss.item())\n",
    "        opti.zero_grad()\n",
    "        loss.backward()\n",
    "        opti.step()\n",
    "        #print(model(batch_obs[0:3]).loc)\n",
    "        #losses[i][k] = loss.item()\n",
    "    #losses = np.array(losses)/num_batches\n",
    "#print(losses)\n",
    "print(losses)\n",
    "torch.save(model,f'./temp_policy/tan_h/final6x_{num_obs}_{n_epochs}_{batch_size}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(pi,\"temp_warthog.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4740f7e2305a459a935d8d587c8fcbc1b1d6d7caacdc0daf6a3c40a8f96c94af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
