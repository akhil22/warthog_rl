{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlagents_envs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-95605983464f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlagents_envs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym_unity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnityToGymWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlagents_envs'"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from gym_unity.envs import UnityToGymWrapper\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(worker_id = 0)\n",
    "envg = UnityToGymWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = PPO('MlpPolicy', envg, verbose=1)\n",
    "model = PPO('MlpPolicy', envg, verbose=1)\n",
    "model = PPO.load('model1.zip')\n",
    "model.env = model1.env\n",
    "act1 = []\n",
    "act2 = []\n",
    "reward = 0\n",
    "with torch.no_grad():\n",
    "    model.policy.eval()\n",
    "    #envg = model.get_env()\n",
    "    obs = envg.reset()\n",
    "    t1 = time.time()\n",
    "    for i in range(5000):\n",
    "        t2 = time.time()\n",
    "        #action, _states = model.predict(obs, deterministic=False) \n",
    "        action, _states = model.predict(obs)\n",
    "        print(action)\n",
    "        act1.append(np.clip(action[0], 0 ,1)*4)\n",
    "        #act2.append(np.clip(action[1], -1 ,1)*2.5)\n",
    "        act2.append(reward) \n",
    "        #action[0] = np.clip(action[0], 0, 1)*4\n",
    "        #action[1] = np.clip(action[1], -1, 1)*2.5\n",
    "        obs, reward, done, info = envg.step(action)\n",
    "        #print(t2-t1)\n",
    "        #if t2 -t1 < 0.3:\n",
    "           # time.sleep(0.3 - (t2-t1))\n",
    "        #t1 = t2\n",
    "    #print(action)\n",
    "    #env.render()\n",
    "        if done:\n",
    "          obs = envg.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "plt.figure()\n",
    "plt.plot(act1, \"*\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(act2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing uncomment\n",
    "model1 = PPO('MlpPolicy', envg, verbose=1)\n",
    "#for learning uncomment\n",
    "model = PPO('MlpPolicy', envg, verbose=1)\n",
    "# model.load('./first_pytorch_multiplication_reward.zip')\n",
    "model = PPO.load('model1.zip')\n",
    "model.env = model1.env\n",
    "model.learn(total_timesteps=1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', envg, verbose=1)\n",
    "model.learn(total_timesteps=1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "#model = PPO.load('./first_pytorch_multiplication_without_angle_reward.zip')\n",
    "#model = PPO.load('first_pytorch_multiplication_reward.zip')\n",
    "model = PPO.load('model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.env = model1.env\n",
    "model.learn(total_timesteps=1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_unity.envs import UnityToGymWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envg = UnityToGymWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = envg.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envg.step([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(1):\n",
    "    envg.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(1):\n",
    "    obs = envg.step([1,1])\n",
    "    print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envg.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', envg, verbose=1)\n",
    "obs = envg.reset()\n",
    "for i in range(5000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action[0] = np.clip(action[0], 0, 1)*4\n",
    "    action[1] = np.clip(action[1], -1, 1)*2.75\n",
    "    obs, reward, done, info = envg.step(action)\n",
    "    print(action)\n",
    "    #env.render()\n",
    "    if done:\n",
    "      obs = envg.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
