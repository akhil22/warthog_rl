{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib widget\n",
    "import torch\n",
    "import gym\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import Normal\n",
    "from env.WarthogEnv import WarthogEnv\n",
    "import scipy.signal\n",
    "import time\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "#torch.manual_seed(100)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, act = nn.ReLU):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [1]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.v = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkCat(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension, act= nn.ReLU):\n",
    "        super(PolicyNetworkCat, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.pi = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        score = self.pi(x)\n",
    "        #probs = F.softmax(score,dim = 1)\n",
    "        dist = torch.distributions.Categorical(logits=score)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkGauss(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension,act = nn.ReLU):\n",
    "        super(PolicyNetworkGauss, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.mu = nn.Sequential(*self.layers)\n",
    "        log_std = -0.5*np.ones(action_dimension, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std), requires_grad=True)\n",
    "    def forward(self, x):\n",
    "        mean = self.mu(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x42 and 22x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor([\u001b[39m0.5\u001b[39m, \u001b[39m0.3\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(o)\n\u001b[1;32m---> 11\u001b[0m m \u001b[39m=\u001b[39m pi(o)\n\u001b[0;32m     12\u001b[0m logp \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mlog_prob(a)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(a\u001b[39m.\u001b[39mshape))\n",
      "File \u001b[1;32mc:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mPolicyNetworkGauss.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmu(x)\n\u001b[0;32m     15\u001b[0m     std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_std)\n\u001b[0;32m     16\u001b[0m     dist \u001b[39m=\u001b[39m Normal(mean, std)\n",
      "File \u001b[1;32mc:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\akhil\\.conda\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x42 and 22x128)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "#env = gym.make('CartPole-v1')\n",
    "env = WarthogEnv('sim_remote_waypoint.txt', None)\n",
    "sizes = [128]\n",
    "obs_dimension = env.observation_space.shape\n",
    "action_dimension = env.action_space.shape\n",
    "pi = PolicyNetworkGauss(*obs_dimension, sizes, *action_dimension)\n",
    "o = torch.zeros(1,42)\n",
    "a = torch.as_tensor([0.5, 0.3], dtype=torch.float32)\n",
    "print(o)\n",
    "m = pi(o)\n",
    "logp = m.log_prob(a)\n",
    "print(len(a.shape))\n",
    "print(m)\n",
    "print(logp)\n",
    "print(m.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        #self.act_buf = np.zeros((size,), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size,act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "       # print(rews)\n",
    "       # print(vals)\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "\n",
    "        self.adv_buf = (self.adv_buf - self.adv_buf.mean()) / (self.adv_buf.std()+eps)\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env, seed = 0, steps = 128, buff_size = 4, train_time_steps = 10000000, gamma = 0.99, clip_ratio = 0.2, lr_pi = 3e-5, \n",
    "        lr_vf = 1e-3, pi_train_itrs = 4, v_train_itrs = 4, lam = 0.97, max_ep_len = 500):\n",
    "        obs_dim = env.observation_space.shape\n",
    "        #action_dim = 2\n",
    "        action_dim = env.action_space.shape\n",
    "        h_sizes = [64,64]\n",
    "        vi = ValueNetwork(*obs_dim, h_sizes, act=nn.Tanh).to(device)\n",
    "        #pi = PolicyNetworkCat(*obs_dim, h_sizes, action_dim).to(device)\n",
    "        pi = PolicyNetworkGauss(*obs_dim, h_sizes, *action_dim,act=nn.Tanh).to(device)\n",
    "        data_buff = PPOBuffer(*obs_dim, *action_dim, steps)\n",
    "        policy_opt = optim.Adam(pi.parameters(), lr = lr_pi)\n",
    "        value_opt = optim.Adam(vi.parameters(), lr = lr_vf)\n",
    "        obs = env.reset()\n",
    "        curr_time_step = 0\n",
    "        pbar = tqdm(total = train_time_steps)\n",
    "        num_episode = 0\n",
    "        ep_rewards = [0]\n",
    "        ep_steps = [0]\n",
    "        start_time = time.time()\n",
    "        while curr_time_step < train_time_steps: \n",
    "                for t in range(0, steps):\n",
    "                        curr_time_step+=1\n",
    "                        with torch.no_grad():\n",
    "                                m = pi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                action = m.sample()\n",
    "                                #print(action)\n",
    "                                action = action.reshape((-1,) + action_dim)\n",
    "                                #print(action)\n",
    "                                logp = m.log_prob(action).sum(dim=1)\n",
    "                               # print(logp)\n",
    "                                action = action.cpu().numpy() \n",
    "                                clipped_action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "                                #print(clipped_action)\n",
    "                                #obs_new, rew, done, _ = env.step(a.item())\n",
    "                                obs_new, rew, done, _ = env.step(clipped_action[0])\n",
    "                                ep_rewards[num_episode]+=rew \n",
    "                                ep_steps[num_episode]+=1\n",
    "                                v = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                        data_buff.store(obs, action, rew, v.cpu().numpy(), logp.cpu().numpy())\n",
    "                        obs = obs_new\n",
    "                        if done or t == steps-1:\n",
    "                                if done:\n",
    "                                        v_ = 0.\n",
    "                                        obs = env.reset()\n",
    "                                        done = False\n",
    "                                        num_episode+=1\n",
    "                                        ep_rewards.append(0)\n",
    "                                        ep_steps.append(0)\n",
    "                                        curr_time = time.time()\n",
    "                                        if num_episode %100 == 0:\n",
    "                                                print(f'episode: {num_episode-1} \\t episode_reward: {np.mean(ep_rewards[-10:-2])} \\t total steps:{curr_time_step}\\t fps\"{curr_time_step/(curr_time-start_time)}\\t avg_ep_steps: {np.mean(ep_steps[-10:-2])}')\n",
    "                                                torch.save(pi, f'./temp_policy/tan_h/manaul2_ppo_batch_{curr_time_step}_rew_{np.mean(ep_rewards[-10:-2])}.pt')\n",
    "                                else:\n",
    "                                        v_ = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                        v_ = v_.detach().cpu().numpy()\n",
    "                                data_buff.finish_path(v_)\n",
    "                        if(curr_time_step % 100000 == 0):\n",
    "                                np.savetxt(f'./temp_policy/tan_h/avg_rew_{curr_time_step}', ep_rewards, fmt='%f')\n",
    "\n",
    "                        #curr_time_step+=1\n",
    "                      #  pbar.update(1)\n",
    "                data = data_buff.get()\n",
    "                ret, act, adv, o, logp_old= data['ret'].to(device), data['act'].to(device), data['adv'].to(device), data['obs'].to(device), data['logp'].to(device)\n",
    "                for j in range(0, pi_train_itrs):\n",
    "                        indices = np.random.permutation(steps)\n",
    "                        start_idx = 0\n",
    "                        while start_idx < steps:\n",
    "\n",
    "                                batch_ind = indices[start_idx: start_idx+buff_size]\n",
    "                                ret_b = ret[batch_ind]\n",
    "                                act_b = act[batch_ind]\n",
    "                                adv_b = adv[batch_ind]\n",
    "                                o_b = o[batch_ind]\n",
    "                           #     o_b_v = o[batch_ind]\n",
    "                                logp_old_b = logp_old[batch_ind]\n",
    "                                act_dist = pi(o_b)\n",
    "                                logp = act_dist.log_prob(act_b).sum(dim=1)\n",
    "                                ratio = torch.exp(logp - logp_old_b)\n",
    "                                clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv_b\n",
    "                                loss_pi = -(torch.min(ratio * adv_b, clip_adv)).mean()\n",
    "                                policy_opt.zero_grad()\n",
    "                                loss_pi.backward()\n",
    "                                policy_opt.step()\n",
    "                       # ret, ob = data['ret'], data['obs']\n",
    "                                val = vi(o_b)\n",
    "                                value_loss = F.mse_loss(val.flatten(), ret_b)\n",
    "                                value_opt.zero_grad()\n",
    "                                value_loss.backward()\n",
    "                                value_opt.step()\n",
    "                                start_idx = start_idx+buff_size\n",
    "                                '''print(ret_b)\n",
    "                                print(act_b)\n",
    "                                print(adv_b)\n",
    "                                print(o_b)\n",
    "                                print(logp_old_b)'''\n",
    "                '''for j in range(0, pi_train_itrs):\n",
    "                        act_dist = pi(o)\n",
    "                        logp = act_dist.log_prob(act).sum(dim=1)\n",
    "                        ratio = torch.exp(logp - logp_old)\n",
    "                        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "                        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "                        loss_pi.backward()\n",
    "                        policy_opt.step()\n",
    "                for i in range (0, v_train_itrs):\n",
    "                        value_opt.zero_grad()\n",
    "                       # ret, ob = data['ret'], data['obs']\n",
    "                        val = vi(o)\n",
    "                        value_loss = F.mse_loss(val.flatten(), ret)\n",
    "                        value_loss.backward()\n",
    "                        value_opt.step()'''\n",
    "                #pbar.update(1)\n",
    "        pbar.close()\n",
    "        return pi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 99 \t episode_reward: 2.7672881921041252 \t total steps:3946\t fps\"460.3613031254579\t avg_ep_steps: 52.75\n",
      "episode: 199 \t episode_reward: 7.477256517879281 \t total steps:7738\t fps\"455.06838228951545\t avg_ep_steps: 41.125\n",
      "episode: 299 \t episode_reward: -1.2829074928837354 \t total steps:12421\t fps\"455.18063363756266\t avg_ep_steps: 51.625\n",
      "episode: 399 \t episode_reward: 607.0254787509293 \t total steps:33066\t fps\"458.53594755752295\t avg_ep_steps: 612.5\n",
      "episode: 499 \t episode_reward: 1020.9124956659309 \t total steps:91490\t fps\"462.54579231619186\t avg_ep_steps: 565.625\n",
      "episode: 599 \t episode_reward: 887.5491427134907 \t total steps:149100\t fps\"465.6362654963244\t avg_ep_steps: 489.875\n",
      "episode: 699 \t episode_reward: 1423.8117564559202 \t total steps:204400\t fps\"464.05872859234427\t avg_ep_steps: 612.5\n",
      "episode: 799 \t episode_reward: 971.2977132055539 \t total steps:267400\t fps\"463.5574834033253\t avg_ep_steps: 588.0\n",
      "episode: 899 \t episode_reward: 1471.9275137805414 \t total steps:327650\t fps\"463.7754325996552\t avg_ep_steps: 700.0\n",
      "episode: 999 \t episode_reward: 1168.7402993608687 \t total steps:382200\t fps\"464.5169433299736\t avg_ep_steps: 437.5\n",
      "episode: 1099 \t episode_reward: 1427.8876368607482 \t total steps:445200\t fps\"464.8430931232191\t avg_ep_steps: 563.125\n",
      "episode: 1199 \t episode_reward: 1409.7940749103432 \t total steps:506100\t fps\"465.0283887290908\t avg_ep_steps: 524.25\n",
      "episode: 1299 \t episode_reward: 2615.070649420556 \t total steps:568400\t fps\"464.58134216844803\t avg_ep_steps: 700.0\n",
      "episode: 1399 \t episode_reward: 2398.634022509359 \t total steps:631400\t fps\"464.6738433686504\t avg_ep_steps: 612.5\n",
      "episode: 1499 \t episode_reward: 3396.9487384643403 \t total steps:695800\t fps\"464.71956736048895\t avg_ep_steps: 700.0\n",
      "episode: 1599 \t episode_reward: 3031.1587171082 \t total steps:761600\t fps\"464.7373970665837\t avg_ep_steps: 612.5\n",
      "episode: 1699 \t episode_reward: 3803.7158440261496 \t total steps:828100\t fps\"464.81594689980403\t avg_ep_steps: 700.0\n",
      "episode: 1799 \t episode_reward: 3755.5719193931704 \t total steps:889000\t fps\"464.887398687908\t avg_ep_steps: 700.0\n",
      "episode: 1899 \t episode_reward: 3005.909966743535 \t total steps:952662\t fps\"464.7676885052719\t avg_ep_steps: 542.0\n",
      "episode: 1999 \t episode_reward: 4322.801220652151 \t total steps:1018500\t fps\"465.2016838860214\t avg_ep_steps: 700.0\n",
      "episode: 2099 \t episode_reward: 3465.863208332359 \t total steps:1080100\t fps\"465.93904393273306\t avg_ep_steps: 533.5\n",
      "episode: 2199 \t episode_reward: 4127.26505362623 \t total steps:1145200\t fps\"466.79153776175167\t avg_ep_steps: 700.0\n",
      "episode: 2299 \t episode_reward: 4398.031765399547 \t total steps:1207500\t fps\"467.41608281097945\t avg_ep_steps: 700.0\n",
      "episode: 2399 \t episode_reward: 4019.7883934171514 \t total steps:1269100\t fps\"468.13387620958304\t avg_ep_steps: 612.5\n",
      "episode: 2499 \t episode_reward: 3272.3505954658485 \t total steps:1333500\t fps\"468.76463925306695\t avg_ep_steps: 612.5\n",
      "episode: 2599 \t episode_reward: 4274.324709238336 \t total steps:1398600\t fps\"469.2621031636884\t avg_ep_steps: 700.0\n",
      "episode: 2699 \t episode_reward: 3631.73675604449 \t total steps:1460200\t fps\"469.3823069669241\t avg_ep_steps: 612.5\n",
      "episode: 2799 \t episode_reward: 4292.186039157961 \t total steps:1523900\t fps\"468.9249173270229\t avg_ep_steps: 700.0\n",
      "episode: 2899 \t episode_reward: 3794.966272753725 \t total steps:1585500\t fps\"468.9108650651156\t avg_ep_steps: 612.5\n",
      "episode: 2999 \t episode_reward: 4398.840882047015 \t total steps:1650600\t fps\"468.7210115885322\t avg_ep_steps: 700.0\n",
      "episode: 3099 \t episode_reward: 3744.7105527960193 \t total steps:1710100\t fps\"468.29986738424606\t avg_ep_steps: 612.5\n",
      "episode: 3199 \t episode_reward: 4588.279438293619 \t total steps:1773800\t fps\"468.5003258159913\t avg_ep_steps: 700.0\n",
      "episode: 3299 \t episode_reward: 4324.474179566427 \t total steps:1841000\t fps\"468.4602175607751\t avg_ep_steps: 700.0\n",
      "episode: 3399 \t episode_reward: 3861.4013337907395 \t total steps:1904700\t fps\"468.0810756265468\t avg_ep_steps: 612.5\n",
      "episode: 3499 \t episode_reward: 3816.7233955582165 \t total steps:1966176\t fps\"467.69908213169145\t avg_ep_steps: 612.5\n",
      "episode: 3599 \t episode_reward: 2777.1894960514473 \t total steps:2027013\t fps\"467.3061415222202\t avg_ep_steps: 437.5\n",
      "episode: 3699 \t episode_reward: 4571.195133537183 \t total steps:2093700\t fps\"466.80375436042846\t avg_ep_steps: 700.0\n",
      "episode: 3799 \t episode_reward: 3970.41219700248 \t total steps:2159655\t fps\"466.7398282254825\t avg_ep_steps: 700.0\n",
      "episode: 3899 \t episode_reward: 4485.7508492799125 \t total steps:2223200\t fps\"467.1791616297399\t avg_ep_steps: 700.0\n",
      "episode: 3999 \t episode_reward: 4284.021151172601 \t total steps:2286200\t fps\"467.62512848970783\t avg_ep_steps: 676.875\n",
      "episode: 4099 \t episode_reward: 4145.470147042641 \t total steps:2351300\t fps\"467.9354732726859\t avg_ep_steps: 614.625\n",
      "episode: 4199 \t episode_reward: 3171.111421827655 \t total steps:2412900\t fps\"468.0465222932183\t avg_ep_steps: 525.0\n",
      "episode: 4299 \t episode_reward: 2989.788934909714 \t total steps:2476997\t fps\"467.9267189768752\t avg_ep_steps: 525.0\n",
      "episode: 4399 \t episode_reward: 3688.398706494346 \t total steps:2541000\t fps\"467.7510835319945\t avg_ep_steps: 612.5\n",
      "episode: 4499 \t episode_reward: 3715.8175627357878 \t total steps:2606100\t fps\"467.831169877756\t avg_ep_steps: 700.0\n",
      "episode: 4599 \t episode_reward: 3754.8236480910186 \t total steps:2669800\t fps\"468.0141040941824\t avg_ep_steps: 612.5\n",
      "episode: 4699 \t episode_reward: 4451.636973810563 \t total steps:2737000\t fps\"468.067430777871\t avg_ep_steps: 700.0\n",
      "episode: 4799 \t episode_reward: 4221.070014386458 \t total steps:2800700\t fps\"468.0538152679343\t avg_ep_steps: 700.0\n",
      "episode: 4899 \t episode_reward: 4730.074402681803 \t total steps:2865200\t fps\"467.9810416144724\t avg_ep_steps: 700.0\n",
      "episode: 4999 \t episode_reward: 4324.641757426018 \t total steps:2929669\t fps\"468.0045378145168\t avg_ep_steps: 700.0\n",
      "episode: 5099 \t episode_reward: 3908.2808629136016 \t total steps:2991100\t fps\"467.9919020939957\t avg_ep_steps: 612.5\n",
      "episode: 5199 \t episode_reward: 3229.470961995014 \t total steps:3057600\t fps\"467.9936370152855\t avg_ep_steps: 612.5\n",
      "episode: 5299 \t episode_reward: 4062.221409466742 \t total steps:3122000\t fps\"468.06380754240257\t avg_ep_steps: 700.0\n",
      "episode: 5399 \t episode_reward: 3509.472943370084 \t total steps:3186400\t fps\"468.04073842590554\t avg_ep_steps: 612.5\n",
      "episode: 5499 \t episode_reward: 3292.26941300068 \t total steps:3251500\t fps\"468.03069174394363\t avg_ep_steps: 556.75\n",
      "episode: 5599 \t episode_reward: 4333.813833570521 \t total steps:3315900\t fps\"468.0663841295736\t avg_ep_steps: 699.875\n",
      "episode: 5699 \t episode_reward: 3794.383957908347 \t total steps:3378200\t fps\"468.064259263932\t avg_ep_steps: 612.5\n",
      "episode: 5799 \t episode_reward: 3278.4733531076577 \t total steps:3443300\t fps\"468.12715743401446\t avg_ep_steps: 527.125\n",
      "episode: 5899 \t episode_reward: 3697.3987086219604 \t total steps:3508400\t fps\"468.3683663940602\t avg_ep_steps: 623.75\n",
      "episode: 5999 \t episode_reward: 3827.6514712017733 \t total steps:3572100\t fps\"468.6109443551791\t avg_ep_steps: 700.0\n",
      "episode: 6099 \t episode_reward: 3498.9272183490098 \t total steps:3636500\t fps\"468.81189828588816\t avg_ep_steps: 612.5\n",
      "episode: 6199 \t episode_reward: 3606.2616475143686 \t total steps:3698800\t fps\"469.0296003551325\t avg_ep_steps: 612.5\n",
      "episode: 6299 \t episode_reward: 3664.6122191621434 \t total steps:3758300\t fps\"469.2220031989944\t avg_ep_steps: 612.5\n",
      "episode: 6399 \t episode_reward: 4580.377144104149 \t total steps:3824800\t fps\"469.44108770547103\t avg_ep_steps: 700.0\n",
      "episode: 6499 \t episode_reward: 3658.5341930466875 \t total steps:3892000\t fps\"469.638437352309\t avg_ep_steps: 612.5\n",
      "episode: 6599 \t episode_reward: 4287.119680327163 \t total steps:3957100\t fps\"469.84714487970905\t avg_ep_steps: 700.0\n",
      "episode: 6699 \t episode_reward: 3614.740803656772 \t total steps:4022900\t fps\"470.02980280728036\t avg_ep_steps: 612.5\n",
      "episode: 6799 \t episode_reward: 3818.080730714324 \t total steps:4085900\t fps\"470.1735945801661\t avg_ep_steps: 612.5\n",
      "episode: 6899 \t episode_reward: 3377.127946340463 \t total steps:4149600\t fps\"470.321259050332\t avg_ep_steps: 525.0\n",
      "episode: 6999 \t episode_reward: 3984.9162266770913 \t total steps:4211200\t fps\"470.5048444967721\t avg_ep_steps: 612.5\n",
      "episode: 7099 \t episode_reward: 3370.4221358213617 \t total steps:4273500\t fps\"470.6452875061925\t avg_ep_steps: 521.25\n",
      "episode: 7199 \t episode_reward: 3957.8976810076347 \t total steps:4336500\t fps\"470.77737549748053\t avg_ep_steps: 612.5\n",
      "episode: 7299 \t episode_reward: 4459.008336485602 \t total steps:4403000\t fps\"470.95443260101064\t avg_ep_steps: 700.0\n",
      "episode: 7399 \t episode_reward: 3962.6034086752516 \t total steps:4467400\t fps\"471.1203648379617\t avg_ep_steps: 612.5\n",
      "episode: 7499 \t episode_reward: 3550.1021421939718 \t total steps:4526200\t fps\"471.2443810549914\t avg_ep_steps: 606.0\n",
      "episode: 7599 \t episode_reward: 3396.37534067889 \t total steps:4589900\t fps\"471.365342510591\t avg_ep_steps: 612.5\n",
      "episode: 7699 \t episode_reward: 2815.5580878727656 \t total steps:4656400\t fps\"471.52308316768784\t avg_ep_steps: 525.0\n",
      "episode: 7799 \t episode_reward: 3565.938581225182 \t total steps:4721500\t fps\"471.67658070402405\t avg_ep_steps: 612.5\n",
      "episode: 7899 \t episode_reward: 4026.1998676093363 \t total steps:4783800\t fps\"471.80017609225905\t avg_ep_steps: 700.0\n",
      "episode: 7999 \t episode_reward: 4570.549235100323 \t total steps:4848200\t fps\"471.9340013092368\t avg_ep_steps: 700.0\n",
      "episode: 8099 \t episode_reward: 4361.953305056331 \t total steps:4909800\t fps\"472.06484601759354\t avg_ep_steps: 700.0\n",
      "episode: 8199 \t episode_reward: 3662.4195243693644 \t total steps:4972100\t fps\"472.1488660190347\t avg_ep_steps: 612.5\n",
      "episode: 8299 \t episode_reward: 4003.701575419436 \t total steps:5030900\t fps\"472.2592294724373\t avg_ep_steps: 612.5\n",
      "episode: 8399 \t episode_reward: 3422.0518601619633 \t total steps:5086900\t fps\"472.3450459660059\t avg_ep_steps: 612.5\n",
      "episode: 8499 \t episode_reward: 2600.89177746482 \t total steps:5145000\t fps\"472.4534274236835\t avg_ep_steps: 525.0\n",
      "episode: 8599 \t episode_reward: 2914.0553825163984 \t total steps:5202400\t fps\"472.54761761685324\t avg_ep_steps: 562.625\n",
      "episode: 8699 \t episode_reward: 2838.6431126543744 \t total steps:5264700\t fps\"472.6384447694664\t avg_ep_steps: 525.0\n",
      "episode: 8799 \t episode_reward: 3334.3240968213604 \t total steps:5325600\t fps\"472.7471118772263\t avg_ep_steps: 588.0\n",
      "episode: 8899 \t episode_reward: 3489.585985541231 \t total steps:5389300\t fps\"472.8520719221864\t avg_ep_steps: 612.5\n",
      "episode: 8999 \t episode_reward: 3983.2851553986247 \t total steps:5455709\t fps\"472.94716749322976\t avg_ep_steps: 700.0\n",
      "episode: 9099 \t episode_reward: 4007.0351415084147 \t total steps:5518100\t fps\"473.04715141852625\t avg_ep_steps: 700.0\n",
      "episode: 9199 \t episode_reward: 3160.8943396034665 \t total steps:5582500\t fps\"473.1630882606544\t avg_ep_steps: 525.0\n",
      "episode: 9299 \t episode_reward: 3767.852347278582 \t total steps:5642000\t fps\"473.25216804082527\t avg_ep_steps: 612.5\n",
      "episode: 9399 \t episode_reward: 4337.890540640368 \t total steps:5705000\t fps\"473.31917657588195\t avg_ep_steps: 700.0\n",
      "episode: 9499 \t episode_reward: 3522.6383748069447 \t total steps:5769400\t fps\"473.40663042399893\t avg_ep_steps: 612.5\n",
      "episode: 9599 \t episode_reward: 4212.197458483501 \t total steps:5838000\t fps\"473.494729246275\t avg_ep_steps: 700.0\n",
      "episode: 9699 \t episode_reward: 4004.5342358464395 \t total steps:5901700\t fps\"473.5595279274255\t avg_ep_steps: 700.0\n",
      "episode: 9799 \t episode_reward: 3682.121192383167 \t total steps:5964700\t fps\"473.6459963534374\t avg_ep_steps: 612.5\n",
      "episode: 9899 \t episode_reward: 3665.8934279069445 \t total steps:6025600\t fps\"473.7355357752345\t avg_ep_steps: 583.5\n",
      "episode: 9999 \t episode_reward: 3355.7665052646007 \t total steps:6086927\t fps\"473.78730553497365\t avg_ep_steps: 572.25\n",
      "episode: 10099 \t episode_reward: 3083.606523193139 \t total steps:6148100\t fps\"473.8593970954808\t avg_ep_steps: 525.0\n",
      "episode: 10199 \t episode_reward: 4234.721017034628 \t total steps:6211800\t fps\"473.9597642759858\t avg_ep_steps: 700.0\n",
      "episode: 10299 \t episode_reward: 3639.891014898388 \t total steps:6276900\t fps\"474.0496940936237\t avg_ep_steps: 612.5\n",
      "episode: 10399 \t episode_reward: 3512.629641937149 \t total steps:6338500\t fps\"474.0971555981274\t avg_ep_steps: 577.875\n",
      "episode: 10499 \t episode_reward: 3477.688322889525 \t total steps:6398374\t fps\"474.15960613367656\t avg_ep_steps: 612.5\n",
      "episode: 10599 \t episode_reward: 3484.6989482645013 \t total steps:6458200\t fps\"474.2220545744464\t avg_ep_steps: 612.5\n",
      "episode: 10699 \t episode_reward: 3456.4454120707296 \t total steps:6523300\t fps\"474.28691825816054\t avg_ep_steps: 700.0\n",
      "episode: 10799 \t episode_reward: 3053.1290717973125 \t total steps:6586300\t fps\"474.3350088957638\t avg_ep_steps: 525.0\n",
      "episode: 10899 \t episode_reward: 4223.893626873896 \t total steps:6645800\t fps\"474.3913200842423\t avg_ep_steps: 700.0\n",
      "episode: 10999 \t episode_reward: 3311.249138704463 \t total steps:6703200\t fps\"474.4479328742689\t avg_ep_steps: 612.5\n",
      "episode: 11099 \t episode_reward: 3899.0798259769053 \t total steps:6763400\t fps\"474.5024777390055\t avg_ep_steps: 700.0\n",
      "episode: 11199 \t episode_reward: 3029.4301050056697 \t total steps:6824300\t fps\"474.54726604078587\t avg_ep_steps: 525.0\n",
      "episode: 11299 \t episode_reward: 4338.82761415164 \t total steps:6890800\t fps\"474.60676645895273\t avg_ep_steps: 700.0\n",
      "episode: 11399 \t episode_reward: 3783.969239789374 \t total steps:6957300\t fps\"474.64797683194126\t avg_ep_steps: 612.5\n",
      "episode: 11499 \t episode_reward: 3911.58483540895 \t total steps:7018900\t fps\"474.69734884799163\t avg_ep_steps: 612.5\n",
      "episode: 11599 \t episode_reward: 4073.4734145123157 \t total steps:7079100\t fps\"474.7482865549772\t avg_ep_steps: 612.5\n",
      "episode: 11699 \t episode_reward: 4177.934417756942 \t total steps:7145600\t fps\"474.82356343202713\t avg_ep_steps: 700.0\n",
      "episode: 11799 \t episode_reward: 3168.0442985691343 \t total steps:7204867\t fps\"474.850114694392\t avg_ep_steps: 525.0\n",
      "episode: 11899 \t episode_reward: 2716.358976072905 \t total steps:7261843\t fps\"474.8974228833814\t avg_ep_steps: 525.0\n",
      "episode: 11999 \t episode_reward: 4213.312370770491 \t total steps:7320600\t fps\"474.9532040313455\t avg_ep_steps: 700.0\n",
      "episode: 12099 \t episode_reward: 3180.5722124979397 \t total steps:7384300\t fps\"475.0235017739181\t avg_ep_steps: 612.5\n",
      "episode: 12199 \t episode_reward: 3848.372944695689 \t total steps:7442400\t fps\"475.0566038795242\t avg_ep_steps: 612.5\n",
      "episode: 12299 \t episode_reward: 3781.538438149549 \t total steps:7501900\t fps\"475.08300900123476\t avg_ep_steps: 597.5\n",
      "episode: 12399 \t episode_reward: 3338.8156552840246 \t total steps:7562100\t fps\"475.11881804672424\t avg_ep_steps: 612.5\n",
      "episode: 12499 \t episode_reward: 2568.769172678097 \t total steps:7618800\t fps\"475.1569079996999\t avg_ep_steps: 525.0\n",
      "episode: 12599 \t episode_reward: 4157.589042001956 \t total steps:7674211\t fps\"475.19270309898\t avg_ep_steps: 684.625\n",
      "episode: 12699 \t episode_reward: 2540.4509777704952 \t total steps:7725200\t fps\"475.22232319821853\t avg_ep_steps: 437.5\n",
      "episode: 12799 \t episode_reward: 3275.3725633466156 \t total steps:7780500\t fps\"475.2702350310875\t avg_ep_steps: 530.375\n",
      "episode: 12899 \t episode_reward: 3855.4852652413774 \t total steps:7840700\t fps\"475.3271863155176\t avg_ep_steps: 612.5\n",
      "episode: 12999 \t episode_reward: 4285.376929567156 \t total steps:7902300\t fps\"475.36972930486456\t avg_ep_steps: 700.0\n",
      "episode: 13099 \t episode_reward: 3614.7777724379275 \t total steps:7966035\t fps\"475.4008550251864\t avg_ep_steps: 612.5\n",
      "episode: 13199 \t episode_reward: 3973.45356198416 \t total steps:8029000\t fps\"475.4608234492843\t avg_ep_steps: 686.125\n",
      "episode: 13299 \t episode_reward: 3950.690817397508 \t total steps:8089151\t fps\"475.5111580488681\t avg_ep_steps: 700.0\n",
      "episode: 13399 \t episode_reward: 3690.5198828018047 \t total steps:8152200\t fps\"475.55174749269406\t avg_ep_steps: 700.0\n",
      "episode: 13499 \t episode_reward: 4271.183619032093 \t total steps:8217300\t fps\"475.5967890121514\t avg_ep_steps: 700.0\n",
      "episode: 13599 \t episode_reward: 4261.446861840415 \t total steps:8281000\t fps\"475.63908366898164\t avg_ep_steps: 700.0\n",
      "episode: 13699 \t episode_reward: 3952.1944082675664 \t total steps:8338907\t fps\"475.6634141787705\t avg_ep_steps: 649.125\n",
      "episode: 13799 \t episode_reward: 3213.643290992408 \t total steps:8400700\t fps\"475.6997732385045\t avg_ep_steps: 592.625\n",
      "episode: 13899 \t episode_reward: 3112.289513839924 \t total steps:8466500\t fps\"475.73778272578755\t avg_ep_steps: 612.5\n",
      "episode: 13999 \t episode_reward: 3515.720420337044 \t total steps:8526700\t fps\"475.78183875154843\t avg_ep_steps: 612.5\n",
      "episode: 14099 \t episode_reward: 3334.659088920843 \t total steps:8585500\t fps\"475.8068128974619\t avg_ep_steps: 493.625\n",
      "episode: 14199 \t episode_reward: 2137.9181339233937 \t total steps:8634500\t fps\"475.8254767356759\t avg_ep_steps: 370.375\n",
      "episode: 14299 \t episode_reward: 3412.8456699134417 \t total steps:8689100\t fps\"475.85993499313963\t avg_ep_steps: 550.625\n",
      "episode: 14399 \t episode_reward: 4167.975116104719 \t total steps:8745800\t fps\"475.88737201664077\t avg_ep_steps: 700.0\n",
      "episode: 14499 \t episode_reward: 2449.6670423498726 \t total steps:8805300\t fps\"475.91446595438345\t avg_ep_steps: 437.5\n",
      "episode: 14599 \t episode_reward: 4140.2817814486825 \t total steps:8863400\t fps\"475.9312725109772\t avg_ep_steps: 700.0\n",
      "episode: 14699 \t episode_reward: 4180.783539186068 \t total steps:8920800\t fps\"475.9650953682308\t avg_ep_steps: 700.0\n",
      "episode: 14799 \t episode_reward: 3746.555202203923 \t total steps:8975776\t fps\"475.969843649557\t avg_ep_steps: 612.5\n",
      "episode: 14899 \t episode_reward: 3348.3646687083206 \t total steps:9035579\t fps\"475.99840766903833\t avg_ep_steps: 594.5\n",
      "episode: 14999 \t episode_reward: 3856.7449584621218 \t total steps:9091600\t fps\"476.01460838371696\t avg_ep_steps: 700.0\n",
      "episode: 15099 \t episode_reward: 3342.8944633827396 \t total steps:9151100\t fps\"476.05355146021526\t avg_ep_steps: 612.5\n",
      "episode: 15199 \t episode_reward: 3916.8880503381893 \t total steps:9212000\t fps\"476.07586985185776\t avg_ep_steps: 700.0\n",
      "episode: 15299 \t episode_reward: 3208.029077041526 \t total steps:9273600\t fps\"476.0984052435079\t avg_ep_steps: 542.375\n",
      "episode: 15399 \t episode_reward: 3568.6150321084397 \t total steps:9338700\t fps\"476.11408941879324\t avg_ep_steps: 612.5\n",
      "episode: 15499 \t episode_reward: 3697.7125414292805 \t total steps:9400300\t fps\"476.15242668300857\t avg_ep_steps: 612.5\n",
      "episode: 15599 \t episode_reward: 3902.834797335571 \t total steps:9463300\t fps\"476.1733165025517\t avg_ep_steps: 629.0\n",
      "episode: 15699 \t episode_reward: 3560.978859573558 \t total steps:9523500\t fps\"476.1926580250404\t avg_ep_steps: 612.5\n",
      "episode: 15799 \t episode_reward: 3609.229954967593 \t total steps:9583000\t fps\"476.2196224582869\t avg_ep_steps: 612.5\n",
      "episode: 15899 \t episode_reward: 4469.829362873657 \t total steps:9648800\t fps\"476.2508823490264\t avg_ep_steps: 700.0\n",
      "episode: 15999 \t episode_reward: 3662.982586883993 \t total steps:9711800\t fps\"476.26595014503926\t avg_ep_steps: 612.5\n",
      "episode: 16099 \t episode_reward: 3827.6593695326706 \t total steps:9767800\t fps\"476.28995796153356\t avg_ep_steps: 612.5\n",
      "episode: 16199 \t episode_reward: 4007.320151519331 \t total steps:9828138\t fps\"476.31649733521004\t avg_ep_steps: 700.0\n",
      "episode: 16299 \t episode_reward: 3362.376734191141 \t total steps:9887500\t fps\"476.3480541621654\t avg_ep_steps: 612.5\n",
      "episode: 16399 \t episode_reward: 2757.7320802552367 \t total steps:9944900\t fps\"476.36653681170196\t avg_ep_steps: 525.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000000 [5:49:51<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#python.dataScience.textOutputLimit = 0\n",
    "pi = ppo(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.clear()\n",
    "\n",
    "#%matplotlib widget\n",
    "#env = WarthogEnv('sim_remote_waypoint.txt', None)\n",
    "#pi = torch.load(\"./temp_warthog.pt\")\n",
    "#plt.ion()\n",
    "#plt.pause(2)\n",
    "#obs = env.reset()\n",
    "#for i in range(0,400):\n",
    "#    m = pi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "#    action = m.sample()\n",
    "#    obs, rew, done, info = env.step(action.cpu().numpy())\n",
    "#    env.render()\n",
    "#    if done:\n",
    "#        obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(pi,\"temp_warthog.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "122de4a4be327aa1ba4f735a2434ab4bffab9a9a54192dfc85777065cfde1c01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
