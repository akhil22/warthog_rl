{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhil/anaconda3/envs/ppo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/akhil/anaconda3/envs/ppo/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import Normal\n",
    "from env.WarthogEnv import WarthogEnv\n",
    "import scipy.signal\n",
    "import time\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "#torch.manual_seed(100)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, act = nn.ReLU):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [1]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.v = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkCat(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension, act= nn.ReLU):\n",
    "        super(PolicyNetworkCat, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.pi = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        score = self.pi(x)\n",
    "        #probs = F.softmax(score,dim = 1)\n",
    "        dist = torch.distributions.Categorical(logits=score)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkGauss(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension,act = nn.ReLU):\n",
    "        super(PolicyNetworkGauss, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.mu = nn.Sequential(*self.layers)\n",
    "        log_std = -0.5*np.ones(action_dimension, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "    def forward(self, x):\n",
    "        mean = self.mu(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "1\n",
      "Normal(loc: torch.Size([1, 2]), scale: torch.Size([1, 2]))\n",
      "tensor([[-0.8774, -0.6654]], grad_fn=<SubBackward0>)\n",
      "tensor([[-0.0808, -0.1258]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhil/anaconda3/envs/ppo/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "#env = gym.make('CartPole-v1')\n",
    "env = WarthogEnv('sim_remote_waypoint.txt', None)\n",
    "sizes = [128]\n",
    "obs_dimension = env.observation_space.shape\n",
    "action_dimension = env.action_space.shape\n",
    "pi = PolicyNetworkGauss(*obs_dimension, sizes, *action_dimension)\n",
    "o = torch.zeros(1,42)\n",
    "a = torch.as_tensor([0.5, 0.3], dtype=torch.float32)\n",
    "print(o)\n",
    "m = pi(o)\n",
    "logp = m.log_prob(a)\n",
    "print(len(a.shape))\n",
    "print(m)\n",
    "print(logp)\n",
    "print(m.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        #self.act_buf = np.zeros((size,), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size,act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "       # print(rews)\n",
    "       # print(vals)\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "\n",
    "        self.adv_buf = (self.adv_buf - self.adv_buf.mean()) / (self.adv_buf.std()+eps)\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env, seed = 0, buff_size = 4000, train_time_steps = 100000, gamma = 0.99, clip_ratio = 0.2, lr_pi = 3e-4, \n",
    "        lr_vf = 1e-3, pi_train_itrs = 80, v_train_itrs = 80, lam = 0.97, max_ep_len = 500):\n",
    "        obs_dim = env.observation_space.shape\n",
    "        #action_dim = 2\n",
    "        action_dim = env.action_space.shape\n",
    "        h_sizes = [64,64]\n",
    "        vi = ValueNetwork(*obs_dim, h_sizes).to(device)\n",
    "        #pi = PolicyNetworkCat(*obs_dim, h_sizes, action_dim).to(device)\n",
    "        pi = PolicyNetworkGauss(*obs_dim, h_sizes, *action_dim).to(device)\n",
    "        data_buff = PPOBuffer(*obs_dim, *action_dim, buff_size)\n",
    "        policy_opt = optim.Adam(pi.parameters(), lr = lr_pi)\n",
    "        value_opt = optim.Adam(vi.parameters(), lr = lr_vf)\n",
    "        obs = env.reset()\n",
    "        curr_time_step = 0\n",
    "        pbar = tqdm(total = train_time_steps)\n",
    "        num_episode = 0\n",
    "        ep_rewards = [0]\n",
    "        ep_steps = [0]\n",
    "        start_time = time.time()\n",
    "        while curr_time_step < train_time_steps: \n",
    "                for t in range(0, buff_size):\n",
    "                        curr_time_step+=1\n",
    "                        with torch.no_grad():\n",
    "                                m = pi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                action = m.sample()\n",
    "                                #print(action)\n",
    "                                action = action.reshape((-1,) + action_dim)\n",
    "                                #print(action)\n",
    "                                logp = m.log_prob(action).sum(dim=1)\n",
    "                               # print(logp)\n",
    "                                action = action.cpu().numpy() \n",
    "                                clipped_action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "                                #print(clipped_action)\n",
    "                                #obs_new, rew, done, _ = env.step(a.item())\n",
    "                                obs_new, rew, done, _ = env.step(clipped_action[0])\n",
    "                                ep_rewards[num_episode]+=rew \n",
    "                                ep_steps[num_episode]+=1\n",
    "                                v = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                        data_buff.store(obs, action, rew, v.cpu().numpy(), logp.cpu().numpy())\n",
    "                        obs = obs_new\n",
    "                        if done or t == buff_size-1:\n",
    "                                if done:\n",
    "                                        v_ = 0.\n",
    "                                        obs = env.reset()\n",
    "                                        done = False\n",
    "                                        num_episode+=1\n",
    "                                        ep_rewards.append(0)\n",
    "                                        ep_steps.append(0)\n",
    "                                        curr_time = time.time()\n",
    "                                        if num_episode %100 == 0:\n",
    "                                                print(f'episode: {num_episode-1} \\t episode_reward: {np.mean(ep_rewards[-10:-2])} \\t total steps:{curr_time_step}\\t fps\"{curr_time_step/(curr_time-start_time)}\\t avg_ep_steps: {np.mean(ep_steps[-10:-2])}')\n",
    "                                else:\n",
    "                                        v_ = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                        v_ = v_.detach().cpu().numpy()\n",
    "                                data_buff.finish_path(v_)\n",
    "                        #curr_time_step+=1\n",
    "                      #  pbar.update(1)\n",
    "                data = data_buff.get()\n",
    "                ret, act, adv, o, logp_old= data['ret'].to(device), data['act'].to(device), data['adv'].to(device), data['obs'].to(device), data['logp'].to(device)\n",
    "                for j in range(0, pi_train_itrs):\n",
    "                        act_dist = pi(o)\n",
    "                        logp = act_dist.log_prob(act).sum(dim=1)\n",
    "                        ratio = torch.exp(logp - logp_old)\n",
    "                        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "                        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "                        loss_pi.backward()\n",
    "                        policy_opt.step()\n",
    "                for i in range (0, v_train_itrs):\n",
    "                        value_opt.zero_grad()\n",
    "                       # ret, ob = data['ret'], data['obs']\n",
    "                        val = vi(o)\n",
    "                        value_loss = F.mse_loss(val.flatten(), ret)\n",
    "                        value_loss.backward()\n",
    "                        value_opt.step()\n",
    "                #pbar.update(1)\n",
    "        pbar.close()\n",
    "        return pi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 99 \t episode_reward: -0.13622264251828264 \t total steps:6659\t fps\"2420.9174423447557\t avg_ep_steps: 86.625\n",
      "episode: 199 \t episode_reward: -4.423413940959607 \t total steps:13278\t fps\"2310.823846261664\t avg_ep_steps: 80.875\n",
      "episode: 299 \t episode_reward: -6.964528620358183 \t total steps:20627\t fps\"2284.803641999233\t avg_ep_steps: 73.125\n",
      "episode: 399 \t episode_reward: 14.273995614154996 \t total steps:30183\t fps\"2295.94057137377\t avg_ep_steps: 113.375\n",
      "episode: 499 \t episode_reward: 823.2205601854938 \t total steps:74200\t fps\"2284.363869944031\t avg_ep_steps: 612.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:43<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#python.dataScience.textOutputLimit = 0\n",
    "pi = ppo(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1075143975.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [11]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "env.clear()\n",
    "env = WarthogEnv('sim_remote_waypoint.txt', None)\n",
    "obs = env.reset()\n",
    "for i in range(0,400):\n",
    "    action = pi(torch.as_tensor(obs, dtype=float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "122de4a4be327aa1ba4f735a2434ab4bffab9a9a54192dfc85777065cfde1c01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
